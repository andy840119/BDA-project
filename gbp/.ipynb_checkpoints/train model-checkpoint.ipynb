{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MySQLdb\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, StratifiedKFold, KFold\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Merge, Input, Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.externals import joblib\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary(Y):\n",
    "    Y[np.where(Y > 0)] = 1\n",
    "    Y[np.where(Y <= 0)] = 0\n",
    "    Y = Y.astype('int64')\n",
    "    f = np.bincount(Y)\n",
    "    print(f/np.sum(f))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shift(data, shift_offset=2):\n",
    "    g = data.as_matrix(columns=['gbp_gradient']).reshape(-1)\n",
    "    for i in range(1, shift_offset+1):\n",
    "        data['gbp_gradient_p_'+str(i)] = shift(g, i, cval=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('newstitle_gbp(uk-news,world,business,technology,money_2000-2016).csv')\n",
    "data['gbp_gradient'] = np.gradient(data.as_matrix(columns=['gbp']).reshape(-1))\n",
    "get_shift(data, shift_offset=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>gbp</th>\n",
       "      <th>gbp_gradient</th>\n",
       "      <th>gbp_gradient_p_1</th>\n",
       "      <th>gbp_gradient_p_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946656000</td>\n",
       "      <td>Big Macs, small horizons Y2K force outstrips s...</td>\n",
       "      <td>1.615700</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>946742400</td>\n",
       "      <td>Cock-fighting? Yes please Samaritans split ove...</td>\n",
       "      <td>1.615000</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>946828800</td>\n",
       "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
       "      <td>1.626724</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>-0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>946915200</td>\n",
       "      <td>Leader: German sleaze inquiry Burundi peace in...</td>\n",
       "      <td>1.637002</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.005512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>947001600</td>\n",
       "      <td>Women in record South Pole walk Deaths no coin...</td>\n",
       "      <td>1.638331</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.011001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                                              title       gbp  \\\n",
       "0  946656000  Big Macs, small horizons Y2K force outstrips s...  1.615700   \n",
       "1  946742400  Cock-fighting? Yes please Samaritans split ove...  1.615000   \n",
       "2  946828800  Hunters threaten Jospin with new battle of the...  1.626724   \n",
       "3  946915200  Leader: German sleaze inquiry Burundi peace in...  1.637002   \n",
       "4  947001600  Women in record South Pole walk Deaths no coin...  1.638331   \n",
       "\n",
       "   gbp_gradient  gbp_gradient_p_1  gbp_gradient_p_2  \n",
       "0     -0.000700          3.000000          3.000000  \n",
       "1      0.005512         -0.000700          3.000000  \n",
       "2      0.011001          0.005512         -0.000700  \n",
       "3      0.005804          0.011001          0.005512  \n",
       "4      0.005379          0.005804          0.011001  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.480071  0.519929]\n"
     ]
    }
   ],
   "source": [
    "Y = data.as_matrix(columns=['gbp_gradient']).reshape(-1)\n",
    "#Y = shift(Y, -3, cval=3)\n",
    "Y = binary(Y)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4795869  0.5204131]\n",
      "[ 0.4795869  0.5204131]\n"
     ]
    }
   ],
   "source": [
    "X_p = data.as_matrix(columns=['gbp_gradient_p_1', 'gbp_gradient_p_2'])\n",
    "X_p[:,0] = binary(X_p[:,0])\n",
    "X_p[:,1] = binary(X_p[:,1])\n",
    "# X_p = data.as_matrix(columns=['gbp_gradient','gbp_gradient_p_1'])\n",
    "# X_p[:,0] = binary(X_p[:,0])\n",
    "# X_p[:,1] = binary(X_p[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6197x2730 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 476823 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_name = 'title'\n",
    "vectorizer = TfidfVectorizer(min_df=50, ngram_range=(1, 4))\n",
    "X = vectorizer.fit_transform(data[feature_name].tolist())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with_news = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_news:\n",
    "    X = hstack([X, X.power(2), X.power(3), X.power(4), X_p])\n",
    "else:\n",
    "    X = X_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='tanh', input_shape=(X.shape[1],), bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='tanh', bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='tanh', bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='tanh', bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='RMSprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>gbp</th>\n",
       "      <th>gbp_gradient</th>\n",
       "      <th>gbp_gradient_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946656000</td>\n",
       "      <td>1.615700</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>946742400</td>\n",
       "      <td>1.615000</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>946828800</td>\n",
       "      <td>1.626724</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>946915200</td>\n",
       "      <td>1.637002</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>947001600</td>\n",
       "      <td>1.638331</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>947088000</td>\n",
       "      <td>1.647760</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>947174400</td>\n",
       "      <td>1.638294</td>\n",
       "      <td>-0.005630</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>947260800</td>\n",
       "      <td>1.636500</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>947347200</td>\n",
       "      <td>1.636500</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>947433600</td>\n",
       "      <td>1.637037</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>947520000</td>\n",
       "      <td>1.647868</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>947606400</td>\n",
       "      <td>1.647033</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>947692800</td>\n",
       "      <td>1.647693</td>\n",
       "      <td>-0.005462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>947779200</td>\n",
       "      <td>1.636108</td>\n",
       "      <td>-0.007347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>947865600</td>\n",
       "      <td>1.633000</td>\n",
       "      <td>-0.001554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>947952000</td>\n",
       "      <td>1.633000</td>\n",
       "      <td>-0.001257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>948038400</td>\n",
       "      <td>1.630486</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>948124800</td>\n",
       "      <td>1.637738</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>948211200</td>\n",
       "      <td>1.643449</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>948297600</td>\n",
       "      <td>1.654645</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>948384000</td>\n",
       "      <td>1.650359</td>\n",
       "      <td>-0.003523</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>948470400</td>\n",
       "      <td>1.647600</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>948556800</td>\n",
       "      <td>1.648500</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>948643200</td>\n",
       "      <td>1.652200</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>948729600</td>\n",
       "      <td>1.648148</td>\n",
       "      <td>-0.006698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>948816000</td>\n",
       "      <td>1.638805</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>948902400</td>\n",
       "      <td>1.636098</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>948988800</td>\n",
       "      <td>1.620183</td>\n",
       "      <td>-0.007899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>949075200</td>\n",
       "      <td>1.620300</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>949161600</td>\n",
       "      <td>1.620300</td>\n",
       "      <td>-0.001071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6167</th>\n",
       "      <td>1480694400</td>\n",
       "      <td>1.272760</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6168</th>\n",
       "      <td>1480780800</td>\n",
       "      <td>1.272780</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6169</th>\n",
       "      <td>1480867200</td>\n",
       "      <td>1.270852</td>\n",
       "      <td>-0.001103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6170</th>\n",
       "      <td>1480953600</td>\n",
       "      <td>1.270575</td>\n",
       "      <td>-0.005175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>1481040000</td>\n",
       "      <td>1.260501</td>\n",
       "      <td>-0.006492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6172</th>\n",
       "      <td>1481126400</td>\n",
       "      <td>1.257591</td>\n",
       "      <td>-0.001178</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6173</th>\n",
       "      <td>1481212800</td>\n",
       "      <td>1.258145</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>1481299200</td>\n",
       "      <td>1.257300</td>\n",
       "      <td>-0.000522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6175</th>\n",
       "      <td>1481385600</td>\n",
       "      <td>1.257100</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176</th>\n",
       "      <td>1481472000</td>\n",
       "      <td>1.267493</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6177</th>\n",
       "      <td>1481558400</td>\n",
       "      <td>1.268368</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178</th>\n",
       "      <td>1481644800</td>\n",
       "      <td>1.271379</td>\n",
       "      <td>-0.014918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6179</th>\n",
       "      <td>1481731200</td>\n",
       "      <td>1.238533</td>\n",
       "      <td>-0.011847</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6180</th>\n",
       "      <td>1481817600</td>\n",
       "      <td>1.247686</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6181</th>\n",
       "      <td>1481904000</td>\n",
       "      <td>1.248950</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>1481990400</td>\n",
       "      <td>1.249000</td>\n",
       "      <td>-0.003711</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6183</th>\n",
       "      <td>1482076800</td>\n",
       "      <td>1.241528</td>\n",
       "      <td>-0.006678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6184</th>\n",
       "      <td>1482163200</td>\n",
       "      <td>1.235645</td>\n",
       "      <td>-0.003242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6185</th>\n",
       "      <td>1482249600</td>\n",
       "      <td>1.235043</td>\n",
       "      <td>-0.002854</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186</th>\n",
       "      <td>1482336000</td>\n",
       "      <td>1.229937</td>\n",
       "      <td>-0.004875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6187</th>\n",
       "      <td>1482422400</td>\n",
       "      <td>1.225293</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6188</th>\n",
       "      <td>1482508800</td>\n",
       "      <td>1.228500</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6189</th>\n",
       "      <td>1482595200</td>\n",
       "      <td>1.227800</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>1482681600</td>\n",
       "      <td>1.228694</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6191</th>\n",
       "      <td>1482768000</td>\n",
       "      <td>1.226001</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6192</th>\n",
       "      <td>1482854400</td>\n",
       "      <td>1.222100</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6193</th>\n",
       "      <td>1482940800</td>\n",
       "      <td>1.223908</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6194</th>\n",
       "      <td>1483027200</td>\n",
       "      <td>1.233779</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6195</th>\n",
       "      <td>1483113600</td>\n",
       "      <td>1.233210</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6196</th>\n",
       "      <td>1483200000</td>\n",
       "      <td>1.233900</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6197 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp       gbp  gbp_gradient  gbp_gradient_binary\n",
       "0      946656000  1.615700     -0.000700                    0\n",
       "1      946742400  1.615000      0.005512                    1\n",
       "2      946828800  1.626724      0.011001                    1\n",
       "3      946915200  1.637002      0.005804                    1\n",
       "4      947001600  1.638331      0.005379                    1\n",
       "5      947088000  1.647760     -0.000018                    0\n",
       "6      947174400  1.638294     -0.005630                    0\n",
       "7      947260800  1.636500     -0.000897                    0\n",
       "8      947347200  1.636500      0.000268                    1\n",
       "9      947433600  1.637037      0.005684                    1\n",
       "10     947520000  1.647868      0.004998                    1\n",
       "11     947606400  1.647033     -0.000087                    0\n",
       "12     947692800  1.647693     -0.005462                    0\n",
       "13     947779200  1.636108     -0.007347                    0\n",
       "14     947865600  1.633000     -0.001554                    0\n",
       "15     947952000  1.633000     -0.001257                    0\n",
       "16     948038400  1.630486      0.002369                    1\n",
       "17     948124800  1.637738      0.006481                    1\n",
       "18     948211200  1.643449      0.008453                    1\n",
       "19     948297600  1.654645      0.003455                    1\n",
       "20     948384000  1.650359     -0.003523                    0\n",
       "21     948470400  1.647600     -0.000929                    0\n",
       "22     948556800  1.648500      0.002300                    1\n",
       "23     948643200  1.652200     -0.000176                    0\n",
       "24     948729600  1.648148     -0.006698                    0\n",
       "25     948816000  1.638805     -0.006025                    0\n",
       "26     948902400  1.636098     -0.009311                    0\n",
       "27     948988800  1.620183     -0.007899                    0\n",
       "28     949075200  1.620300      0.000058                    1\n",
       "29     949161600  1.620300     -0.001071                    0\n",
       "...          ...       ...           ...                  ...\n",
       "6167  1480694400  1.272760      0.002316                    1\n",
       "6168  1480780800  1.272780     -0.000954                    0\n",
       "6169  1480867200  1.270852     -0.001103                    0\n",
       "6170  1480953600  1.270575     -0.005175                    0\n",
       "6171  1481040000  1.260501     -0.006492                    0\n",
       "6172  1481126400  1.257591     -0.001178                    0\n",
       "6173  1481212800  1.258145     -0.000146                    0\n",
       "6174  1481299200  1.257300     -0.000522                    0\n",
       "6175  1481385600  1.257100      0.005096                    1\n",
       "6176  1481472000  1.267493      0.005634                    1\n",
       "6177  1481558400  1.268368      0.001943                    1\n",
       "6178  1481644800  1.271379     -0.014918                    0\n",
       "6179  1481731200  1.238533     -0.011847                    0\n",
       "6180  1481817600  1.247686      0.005209                    1\n",
       "6181  1481904000  1.248950      0.000657                    1\n",
       "6182  1481990400  1.249000     -0.003711                    0\n",
       "6183  1482076800  1.241528     -0.006678                    0\n",
       "6184  1482163200  1.235645     -0.003242                    0\n",
       "6185  1482249600  1.235043     -0.002854                    0\n",
       "6186  1482336000  1.229937     -0.004875                    0\n",
       "6187  1482422400  1.225293     -0.000719                    0\n",
       "6188  1482508800  1.228500      0.001253                    1\n",
       "6189  1482595200  1.227800      0.000097                    1\n",
       "6190  1482681600  1.228694     -0.000900                    0\n",
       "6191  1482768000  1.226001     -0.003297                    0\n",
       "6192  1482854400  1.222100     -0.001046                    0\n",
       "6193  1482940800  1.223908      0.005839                    1\n",
       "6194  1483027200  1.233779      0.004651                    1\n",
       "6195  1483113600  1.233210      0.000061                    1\n",
       "6196  1483200000  1.233900      0.000690                    1\n",
       "\n",
       "[6197 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame()\n",
    "output['timestamp'] = data['timestamp']\n",
    "output['gbp'] = data['gbp']\n",
    "output['gbp_gradient'] = data['gbp_gradient']\n",
    "output['gbp_gradient_binary'] = Y\n",
    "binary_pred = np.zeros(6197)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_result(X_test, y_test, binary_pred, test_idx):\n",
    "    target_names = ['decline','up']\n",
    "    pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    binary_pred[test_idx] = pred\n",
    "    #print(confusion_matrix(y_test, pred, labels=[0,1]))\n",
    "    mat = confusion_matrix(y_test, pred, labels=[0,1])\n",
    "    report = classification_report(y_test, pred, target_names=target_names) + '\\n'\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4957 samples, validate on 1240 samples\n",
      "Epoch 1/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.7030 - acc: 0.4896 - val_loss: 0.6940 - val_acc: 0.4694\n",
      "Epoch 2/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6996 - acc: 0.4961 - val_loss: 0.6935 - val_acc: 0.4782\n",
      "Epoch 3/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6965 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.4919\n",
      "Epoch 4/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6952 - acc: 0.5037 - val_loss: 0.6927 - val_acc: 0.5113\n",
      "Epoch 5/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6942 - acc: 0.5241 - val_loss: 0.6924 - val_acc: 0.5202\n",
      "Epoch 6/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6959 - acc: 0.4985 - val_loss: 0.6923 - val_acc: 0.5145\n",
      "Epoch 7/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6958 - acc: 0.5074 - val_loss: 0.6921 - val_acc: 0.5153\n",
      "Epoch 8/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6936 - acc: 0.5191 - val_loss: 0.6919 - val_acc: 0.5161\n",
      "Epoch 9/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6952 - acc: 0.5094 - val_loss: 0.6917 - val_acc: 0.5177\n",
      "Epoch 10/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6969 - acc: 0.5072 - val_loss: 0.6916 - val_acc: 0.5137\n",
      "Epoch 11/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6936 - acc: 0.5045 - val_loss: 0.6913 - val_acc: 0.5169\n",
      "Epoch 12/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6931 - acc: 0.5136 - val_loss: 0.6911 - val_acc: 0.5177\n",
      "Epoch 13/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6901 - acc: 0.5231 - val_loss: 0.6906 - val_acc: 0.5202\n",
      "Epoch 14/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6925 - acc: 0.5158 - val_loss: 0.6904 - val_acc: 0.5202\n",
      "Epoch 15/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6907 - acc: 0.5235 - val_loss: 0.6903 - val_acc: 0.5210\n",
      "Epoch 16/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6900 - acc: 0.5275 - val_loss: 0.6900 - val_acc: 0.5226\n",
      "Epoch 17/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6907 - acc: 0.5124 - val_loss: 0.6897 - val_acc: 0.5169\n",
      "Epoch 18/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6898 - acc: 0.5255 - val_loss: 0.6894 - val_acc: 0.5202\n",
      "Epoch 19/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6855 - acc: 0.5419 - val_loss: 0.6890 - val_acc: 0.5250\n",
      "Epoch 20/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6895 - acc: 0.5294 - val_loss: 0.6887 - val_acc: 0.5210\n",
      "Epoch 21/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6854 - acc: 0.5481 - val_loss: 0.6884 - val_acc: 0.5363\n",
      "Epoch 22/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6862 - acc: 0.5396 - val_loss: 0.6881 - val_acc: 0.5379\n",
      "Epoch 23/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6853 - acc: 0.5404 - val_loss: 0.6878 - val_acc: 0.5750\n",
      "Epoch 24/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6834 - acc: 0.5519 - val_loss: 0.6873 - val_acc: 0.5637\n",
      "Epoch 25/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6812 - acc: 0.5519 - val_loss: 0.6866 - val_acc: 0.5452\n",
      "Epoch 26/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6813 - acc: 0.5544 - val_loss: 0.6863 - val_acc: 0.5685\n",
      "Epoch 27/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6795 - acc: 0.5548 - val_loss: 0.6857 - val_acc: 0.5548\n",
      "Epoch 28/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6778 - acc: 0.5711 - val_loss: 0.6854 - val_acc: 0.5750\n",
      "Epoch 29/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6776 - acc: 0.5667 - val_loss: 0.6851 - val_acc: 0.5815\n",
      "Epoch 30/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6740 - acc: 0.5762 - val_loss: 0.6846 - val_acc: 0.5944\n",
      "Epoch 31/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6736 - acc: 0.5772 - val_loss: 0.6840 - val_acc: 0.5911\n",
      "Epoch 32/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6727 - acc: 0.5747 - val_loss: 0.6834 - val_acc: 0.5911\n",
      "Epoch 33/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6736 - acc: 0.5762 - val_loss: 0.6829 - val_acc: 0.6000\n",
      "Epoch 34/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6697 - acc: 0.5834 - val_loss: 0.6823 - val_acc: 0.5927\n",
      "Epoch 35/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6706 - acc: 0.5802 - val_loss: 0.6821 - val_acc: 0.6040\n",
      "Epoch 36/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6666 - acc: 0.5937 - val_loss: 0.6812 - val_acc: 0.5968\n",
      "Epoch 37/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6654 - acc: 0.5893 - val_loss: 0.6807 - val_acc: 0.5952\n",
      "Epoch 38/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6639 - acc: 0.6042 - val_loss: 0.6803 - val_acc: 0.6048\n",
      "Epoch 39/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6631 - acc: 0.5955 - val_loss: 0.6799 - val_acc: 0.6065\n",
      "Epoch 40/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6597 - acc: 0.6135 - val_loss: 0.6793 - val_acc: 0.6073\n",
      "1240/1240 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4957 samples, validate on 1240 samples\n",
      "Epoch 1/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.7057 - acc: 0.4999 - val_loss: 0.6906 - val_acc: 0.5202\n",
      "Epoch 2/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6983 - acc: 0.5023 - val_loss: 0.6901 - val_acc: 0.5202\n",
      "Epoch 3/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6960 - acc: 0.5172 - val_loss: 0.6895 - val_acc: 0.5202\n",
      "Epoch 4/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6966 - acc: 0.5181 - val_loss: 0.6892 - val_acc: 0.5202\n",
      "Epoch 5/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6984 - acc: 0.4999 - val_loss: 0.6886 - val_acc: 0.5202\n",
      "Epoch 6/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6905 - acc: 0.5318 - val_loss: 0.6881 - val_acc: 0.5202\n",
      "Epoch 7/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6921 - acc: 0.5275 - val_loss: 0.6877 - val_acc: 0.5226\n",
      "Epoch 8/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6932 - acc: 0.5130 - val_loss: 0.6872 - val_acc: 0.5266\n",
      "Epoch 9/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6897 - acc: 0.5231 - val_loss: 0.6866 - val_acc: 0.5315\n",
      "Epoch 10/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6898 - acc: 0.5318 - val_loss: 0.6860 - val_acc: 0.5371\n",
      "Epoch 11/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6872 - acc: 0.5384 - val_loss: 0.6854 - val_acc: 0.5355\n",
      "Epoch 12/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6862 - acc: 0.5398 - val_loss: 0.6846 - val_acc: 0.5323\n",
      "Epoch 13/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6852 - acc: 0.5390 - val_loss: 0.6839 - val_acc: 0.5597\n",
      "Epoch 14/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6866 - acc: 0.5382 - val_loss: 0.6832 - val_acc: 0.5621\n",
      "Epoch 15/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6836 - acc: 0.5477 - val_loss: 0.6823 - val_acc: 0.5476\n",
      "Epoch 16/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6827 - acc: 0.5530 - val_loss: 0.6815 - val_acc: 0.5702\n",
      "Epoch 17/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6792 - acc: 0.5532 - val_loss: 0.6807 - val_acc: 0.5919\n",
      "Epoch 18/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6768 - acc: 0.5647 - val_loss: 0.6797 - val_acc: 0.5935\n",
      "Epoch 19/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6823 - acc: 0.5473 - val_loss: 0.6789 - val_acc: 0.6105\n",
      "Epoch 20/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6798 - acc: 0.5554 - val_loss: 0.6783 - val_acc: 0.6347\n",
      "Epoch 21/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6742 - acc: 0.5671 - val_loss: 0.6770 - val_acc: 0.6355\n",
      "Epoch 22/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6736 - acc: 0.5739 - val_loss: 0.6758 - val_acc: 0.6032\n",
      "Epoch 23/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6713 - acc: 0.5812 - val_loss: 0.6750 - val_acc: 0.6371\n",
      "Epoch 24/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6725 - acc: 0.5778 - val_loss: 0.6741 - val_acc: 0.6427\n",
      "Epoch 25/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6681 - acc: 0.5868 - val_loss: 0.6729 - val_acc: 0.6153\n",
      "Epoch 26/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6685 - acc: 0.5850 - val_loss: 0.6722 - val_acc: 0.6379\n",
      "Epoch 27/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6630 - acc: 0.6096 - val_loss: 0.6710 - val_acc: 0.6419\n",
      "Epoch 28/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6657 - acc: 0.5905 - val_loss: 0.6701 - val_acc: 0.6524\n",
      "Epoch 29/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6601 - acc: 0.5917 - val_loss: 0.6686 - val_acc: 0.6419\n",
      "Epoch 30/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6631 - acc: 0.5987 - val_loss: 0.6681 - val_acc: 0.6468\n",
      "Epoch 31/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6594 - acc: 0.6072 - val_loss: 0.6671 - val_acc: 0.6435\n",
      "Epoch 32/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6610 - acc: 0.6102 - val_loss: 0.6663 - val_acc: 0.6460\n",
      "Epoch 33/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6546 - acc: 0.6207 - val_loss: 0.6649 - val_acc: 0.6355\n",
      "Epoch 34/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6511 - acc: 0.6244 - val_loss: 0.6645 - val_acc: 0.6419\n",
      "Epoch 35/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6503 - acc: 0.6381 - val_loss: 0.6632 - val_acc: 0.6419\n",
      "Epoch 36/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6497 - acc: 0.6326 - val_loss: 0.6621 - val_acc: 0.6395\n",
      "Epoch 37/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6469 - acc: 0.6314 - val_loss: 0.6610 - val_acc: 0.6347\n",
      "Epoch 38/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6475 - acc: 0.6183 - val_loss: 0.6605 - val_acc: 0.6395\n",
      "Epoch 39/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6401 - acc: 0.6472 - val_loss: 0.6600 - val_acc: 0.6395\n",
      "Epoch 40/40\n",
      "4957/4957 [==============================] - 0s - loss: 0.6427 - acc: 0.6474 - val_loss: 0.6585 - val_acc: 0.6339\n",
      "1240/1240 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.7117 - acc: 0.5067 - val_loss: 0.6913 - val_acc: 0.5254\n",
      "Epoch 2/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6981 - acc: 0.5129 - val_loss: 0.6907 - val_acc: 0.5222\n",
      "Epoch 3/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6982 - acc: 0.5058 - val_loss: 0.6904 - val_acc: 0.5206\n",
      "Epoch 4/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6935 - acc: 0.5258 - val_loss: 0.6899 - val_acc: 0.5214\n",
      "Epoch 5/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6954 - acc: 0.5202 - val_loss: 0.6895 - val_acc: 0.5222\n",
      "Epoch 6/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6931 - acc: 0.5222 - val_loss: 0.6891 - val_acc: 0.5174\n",
      "Epoch 7/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6923 - acc: 0.5286 - val_loss: 0.6885 - val_acc: 0.5214\n",
      "Epoch 8/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6892 - acc: 0.5262 - val_loss: 0.6880 - val_acc: 0.5222\n",
      "Epoch 9/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6853 - acc: 0.5486 - val_loss: 0.6874 - val_acc: 0.5270\n",
      "Epoch 10/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6865 - acc: 0.5415 - val_loss: 0.6869 - val_acc: 0.5222\n",
      "Epoch 11/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6861 - acc: 0.5361 - val_loss: 0.6864 - val_acc: 0.5270\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4958/4958 [==============================] - 0s - loss: 0.6861 - acc: 0.5438 - val_loss: 0.6858 - val_acc: 0.5311\n",
      "Epoch 13/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6843 - acc: 0.5587 - val_loss: 0.6852 - val_acc: 0.5416\n",
      "Epoch 14/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6816 - acc: 0.5571 - val_loss: 0.6845 - val_acc: 0.5609\n",
      "Epoch 15/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6827 - acc: 0.5510 - val_loss: 0.6837 - val_acc: 0.5383\n",
      "Epoch 16/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6772 - acc: 0.5712 - val_loss: 0.6829 - val_acc: 0.5722\n",
      "Epoch 17/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6771 - acc: 0.5666 - val_loss: 0.6823 - val_acc: 0.5529\n",
      "Epoch 18/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6725 - acc: 0.5722 - val_loss: 0.6813 - val_acc: 0.5868\n",
      "Epoch 19/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6753 - acc: 0.5702 - val_loss: 0.6805 - val_acc: 0.5851\n",
      "Epoch 20/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6698 - acc: 0.5829 - val_loss: 0.6795 - val_acc: 0.6005\n",
      "Epoch 21/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6669 - acc: 0.6010 - val_loss: 0.6787 - val_acc: 0.6166\n",
      "Epoch 22/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6668 - acc: 0.5950 - val_loss: 0.6779 - val_acc: 0.6061\n",
      "Epoch 23/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6665 - acc: 0.5900 - val_loss: 0.6770 - val_acc: 0.6134\n",
      "Epoch 24/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6631 - acc: 0.6069 - val_loss: 0.6761 - val_acc: 0.6174\n",
      "Epoch 25/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6565 - acc: 0.6035 - val_loss: 0.6751 - val_acc: 0.6174\n",
      "Epoch 26/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6566 - acc: 0.6121 - val_loss: 0.6741 - val_acc: 0.6207\n",
      "Epoch 27/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6539 - acc: 0.6176 - val_loss: 0.6733 - val_acc: 0.6199\n",
      "Epoch 28/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6548 - acc: 0.6105 - val_loss: 0.6724 - val_acc: 0.6231\n",
      "Epoch 29/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6490 - acc: 0.6271 - val_loss: 0.6717 - val_acc: 0.6174\n",
      "Epoch 30/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6474 - acc: 0.6311 - val_loss: 0.6708 - val_acc: 0.6223\n",
      "Epoch 31/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6487 - acc: 0.6424 - val_loss: 0.6702 - val_acc: 0.6231\n",
      "Epoch 32/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6455 - acc: 0.6255 - val_loss: 0.6693 - val_acc: 0.6223\n",
      "Epoch 33/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6416 - acc: 0.6430 - val_loss: 0.6688 - val_acc: 0.6231\n",
      "Epoch 34/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6366 - acc: 0.6468 - val_loss: 0.6680 - val_acc: 0.6231\n",
      "Epoch 35/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6371 - acc: 0.6478 - val_loss: 0.6674 - val_acc: 0.6247\n",
      "Epoch 36/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6353 - acc: 0.6497 - val_loss: 0.6667 - val_acc: 0.6215\n",
      "Epoch 37/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6291 - acc: 0.6632 - val_loss: 0.6659 - val_acc: 0.6263\n",
      "Epoch 38/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6309 - acc: 0.6634 - val_loss: 0.6654 - val_acc: 0.6182\n",
      "Epoch 39/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6272 - acc: 0.6612 - val_loss: 0.6653 - val_acc: 0.6247\n",
      "Epoch 40/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6214 - acc: 0.6630 - val_loss: 0.6648 - val_acc: 0.6247\n",
      "1239/1239 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.7416 - acc: 0.5024 - val_loss: 0.6950 - val_acc: 0.5198\n",
      "Epoch 2/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.7149 - acc: 0.5071 - val_loss: 0.6931 - val_acc: 0.5198\n",
      "Epoch 3/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.7080 - acc: 0.5123 - val_loss: 0.6923 - val_acc: 0.5198\n",
      "Epoch 4/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.7045 - acc: 0.4960 - val_loss: 0.6918 - val_acc: 0.5214\n",
      "Epoch 5/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6995 - acc: 0.5173 - val_loss: 0.6912 - val_acc: 0.5295\n",
      "Epoch 6/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6969 - acc: 0.5099 - val_loss: 0.6910 - val_acc: 0.5416\n",
      "Epoch 7/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6976 - acc: 0.5127 - val_loss: 0.6908 - val_acc: 0.5367\n",
      "Epoch 8/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6939 - acc: 0.5171 - val_loss: 0.6903 - val_acc: 0.5383\n",
      "Epoch 9/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6918 - acc: 0.5274 - val_loss: 0.6898 - val_acc: 0.5472\n",
      "Epoch 10/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6914 - acc: 0.5375 - val_loss: 0.6894 - val_acc: 0.5585\n",
      "Epoch 11/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6881 - acc: 0.5282 - val_loss: 0.6888 - val_acc: 0.5617\n",
      "Epoch 12/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6864 - acc: 0.5456 - val_loss: 0.6884 - val_acc: 0.5617\n",
      "Epoch 13/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6879 - acc: 0.5341 - val_loss: 0.6884 - val_acc: 0.5545\n",
      "Epoch 14/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6866 - acc: 0.5349 - val_loss: 0.6878 - val_acc: 0.5722\n",
      "Epoch 15/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6837 - acc: 0.5456 - val_loss: 0.6872 - val_acc: 0.5706\n",
      "Epoch 16/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6817 - acc: 0.5579 - val_loss: 0.6866 - val_acc: 0.5698\n",
      "Epoch 17/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6809 - acc: 0.5619 - val_loss: 0.6861 - val_acc: 0.5730\n",
      "Epoch 18/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6795 - acc: 0.5494 - val_loss: 0.6857 - val_acc: 0.5666\n",
      "Epoch 19/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6781 - acc: 0.5615 - val_loss: 0.6847 - val_acc: 0.5690\n",
      "Epoch 20/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6745 - acc: 0.5647 - val_loss: 0.6840 - val_acc: 0.5666\n",
      "Epoch 21/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6725 - acc: 0.5718 - val_loss: 0.6839 - val_acc: 0.5698\n",
      "Epoch 22/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6721 - acc: 0.5811 - val_loss: 0.6831 - val_acc: 0.5779\n",
      "Epoch 23/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6723 - acc: 0.5752 - val_loss: 0.6826 - val_acc: 0.5779\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4958/4958 [==============================] - 0s - loss: 0.6701 - acc: 0.5698 - val_loss: 0.6819 - val_acc: 0.5819\n",
      "Epoch 25/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6669 - acc: 0.5904 - val_loss: 0.6816 - val_acc: 0.5771\n",
      "Epoch 26/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6658 - acc: 0.5851 - val_loss: 0.6807 - val_acc: 0.5811\n",
      "Epoch 27/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6625 - acc: 0.6000 - val_loss: 0.6799 - val_acc: 0.5803\n",
      "Epoch 28/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6618 - acc: 0.5950 - val_loss: 0.6795 - val_acc: 0.5811\n",
      "Epoch 29/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6626 - acc: 0.5962 - val_loss: 0.6782 - val_acc: 0.5819\n",
      "Epoch 30/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6561 - acc: 0.6069 - val_loss: 0.6778 - val_acc: 0.5908\n",
      "Epoch 31/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6546 - acc: 0.6180 - val_loss: 0.6768 - val_acc: 0.5892\n",
      "Epoch 32/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6566 - acc: 0.6134 - val_loss: 0.6764 - val_acc: 0.5932\n",
      "Epoch 33/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6506 - acc: 0.6083 - val_loss: 0.6756 - val_acc: 0.5924\n",
      "Epoch 34/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6482 - acc: 0.6174 - val_loss: 0.6748 - val_acc: 0.5924\n",
      "Epoch 35/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6469 - acc: 0.6307 - val_loss: 0.6745 - val_acc: 0.5940\n",
      "Epoch 36/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6453 - acc: 0.6246 - val_loss: 0.6736 - val_acc: 0.5900\n",
      "Epoch 37/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6444 - acc: 0.6345 - val_loss: 0.6736 - val_acc: 0.5997\n",
      "Epoch 38/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6397 - acc: 0.6444 - val_loss: 0.6729 - val_acc: 0.5973\n",
      "Epoch 39/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6379 - acc: 0.6390 - val_loss: 0.6721 - val_acc: 0.5900\n",
      "Epoch 40/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6354 - acc: 0.6392 - val_loss: 0.6717 - val_acc: 0.5964\n",
      "1239/1239 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.7229 - acc: 0.5006 - val_loss: 0.6935 - val_acc: 0.4778\n",
      "Epoch 2/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.7028 - acc: 0.5117 - val_loss: 0.6926 - val_acc: 0.5222\n",
      "Epoch 3/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.7066 - acc: 0.4937 - val_loss: 0.6921 - val_acc: 0.5238\n",
      "Epoch 4/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.7029 - acc: 0.5063 - val_loss: 0.6919 - val_acc: 0.5230\n",
      "Epoch 5/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6989 - acc: 0.5141 - val_loss: 0.6916 - val_acc: 0.5222\n",
      "Epoch 6/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6987 - acc: 0.5065 - val_loss: 0.6914 - val_acc: 0.5238\n",
      "Epoch 7/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6972 - acc: 0.5097 - val_loss: 0.6912 - val_acc: 0.5230\n",
      "Epoch 8/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6967 - acc: 0.5121 - val_loss: 0.6909 - val_acc: 0.5214\n",
      "Epoch 9/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6944 - acc: 0.5228 - val_loss: 0.6906 - val_acc: 0.5214\n",
      "Epoch 10/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6933 - acc: 0.5218 - val_loss: 0.6904 - val_acc: 0.5230\n",
      "Epoch 11/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6935 - acc: 0.5278 - val_loss: 0.6901 - val_acc: 0.5230\n",
      "Epoch 12/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6944 - acc: 0.5121 - val_loss: 0.6898 - val_acc: 0.5222\n",
      "Epoch 13/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6898 - acc: 0.5232 - val_loss: 0.6895 - val_acc: 0.5222\n",
      "Epoch 14/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6893 - acc: 0.5391 - val_loss: 0.6893 - val_acc: 0.5222\n",
      "Epoch 15/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6882 - acc: 0.5488 - val_loss: 0.6891 - val_acc: 0.5335\n",
      "Epoch 16/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6882 - acc: 0.5462 - val_loss: 0.6887 - val_acc: 0.5327\n",
      "Epoch 17/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6898 - acc: 0.5278 - val_loss: 0.6883 - val_acc: 0.5303\n",
      "Epoch 18/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6892 - acc: 0.5355 - val_loss: 0.6879 - val_acc: 0.5270\n",
      "Epoch 19/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6878 - acc: 0.5327 - val_loss: 0.6874 - val_acc: 0.5287\n",
      "Epoch 20/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6872 - acc: 0.5317 - val_loss: 0.6868 - val_acc: 0.5335\n",
      "Epoch 21/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6842 - acc: 0.5403 - val_loss: 0.6863 - val_acc: 0.5400\n",
      "Epoch 22/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6846 - acc: 0.5424 - val_loss: 0.6858 - val_acc: 0.5521\n",
      "Epoch 23/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6836 - acc: 0.5353 - val_loss: 0.6850 - val_acc: 0.5545\n",
      "Epoch 24/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6815 - acc: 0.5597 - val_loss: 0.6843 - val_acc: 0.5585\n",
      "Epoch 25/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6807 - acc: 0.5607 - val_loss: 0.6835 - val_acc: 0.5561\n",
      "Epoch 26/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6831 - acc: 0.5403 - val_loss: 0.6829 - val_acc: 0.5819\n",
      "Epoch 27/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6774 - acc: 0.5704 - val_loss: 0.6824 - val_acc: 0.5948\n",
      "Epoch 28/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6749 - acc: 0.5760 - val_loss: 0.6815 - val_acc: 0.5714\n",
      "Epoch 29/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6784 - acc: 0.5484 - val_loss: 0.6807 - val_acc: 0.5908\n",
      "Epoch 30/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6772 - acc: 0.5617 - val_loss: 0.6801 - val_acc: 0.5738\n",
      "Epoch 31/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6725 - acc: 0.5522 - val_loss: 0.6793 - val_acc: 0.5956\n",
      "Epoch 32/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6686 - acc: 0.5843 - val_loss: 0.6786 - val_acc: 0.6287\n",
      "Epoch 33/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6672 - acc: 0.5920 - val_loss: 0.6775 - val_acc: 0.6086\n",
      "Epoch 34/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6716 - acc: 0.5768 - val_loss: 0.6769 - val_acc: 0.6295\n",
      "Epoch 35/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6700 - acc: 0.5793 - val_loss: 0.6760 - val_acc: 0.6182\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4958/4958 [==============================] - 0s - loss: 0.6667 - acc: 0.5815 - val_loss: 0.6750 - val_acc: 0.6255\n",
      "Epoch 37/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6620 - acc: 0.5942 - val_loss: 0.6741 - val_acc: 0.6142\n",
      "Epoch 38/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6617 - acc: 0.5922 - val_loss: 0.6734 - val_acc: 0.6287\n",
      "Epoch 39/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6590 - acc: 0.5978 - val_loss: 0.6725 - val_acc: 0.6328\n",
      "Epoch 40/40\n",
      "4958/4958 [==============================] - 0s - loss: 0.6603 - acc: 0.6010 - val_loss: 0.6717 - val_acc: 0.6190\n",
      "1239/1239 [==============================] - 0s\n",
      "0\n",
      "acc:0.6163\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.62      0.46      0.53       595\n",
      "         up       0.60      0.74      0.66       645\n",
      "\n",
      "avg / total       0.61      0.61      0.60      1240\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.64      0.54      0.59       595\n",
      "         up       0.63      0.72      0.67       645\n",
      "\n",
      "avg / total       0.63      0.63      0.63      1240\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.63      0.53      0.57       595\n",
      "         up       0.62      0.72      0.66       644\n",
      "\n",
      "avg / total       0.63      0.62      0.62      1239\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.59      0.51      0.55       595\n",
      "         up       0.60      0.68      0.64       644\n",
      "\n",
      "avg / total       0.60      0.60      0.59      1239\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.65      0.44      0.53       595\n",
      "         up       0.60      0.79      0.68       644\n",
      "\n",
      "avg / total       0.63      0.62      0.61      1239\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "acc = []\n",
    "num_epochs = 40\n",
    "history = ''\n",
    "for i in range(1): #做數次隨機StratifiedKFold,求出更平均的數值\n",
    "    for train_idx, test_idx in skf.split(X, Y):\n",
    "        print (\"Running Fold\")\n",
    "        model = create_model()\n",
    "        if with_news:\n",
    "            x = X.toarray()\n",
    "        else:\n",
    "            x = X\n",
    "        y = np_utils.to_categorical(Y, num_classes)\n",
    "        report = model.fit(x[train_idx], y[train_idx], \n",
    "                        epochs=num_epochs,\n",
    "                        validation_data =(x[test_idx], y[test_idx]),\n",
    "                        batch_size=7000,\n",
    "                        verbose = 1)\n",
    "        acc.append(model.evaluate(x[test_idx], y[test_idx], batch_size=2000)[1])\n",
    "        history +=show_result(x[test_idx], Y[test_idx], binary_pred, test_idx)\n",
    "    print(i)\n",
    "print('acc:%.4f'%(np.mean(acc)))\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>gbp</th>\n",
       "      <th>gbp_gradient</th>\n",
       "      <th>gbp_gradient_binary</th>\n",
       "      <th>gbp_gradient_binary_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946656000</td>\n",
       "      <td>1.615700</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>946742400</td>\n",
       "      <td>1.615000</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>946828800</td>\n",
       "      <td>1.626724</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>946915200</td>\n",
       "      <td>1.637002</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>947001600</td>\n",
       "      <td>1.638331</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>947088000</td>\n",
       "      <td>1.647760</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>947174400</td>\n",
       "      <td>1.638294</td>\n",
       "      <td>-0.005630</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>947260800</td>\n",
       "      <td>1.636500</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>947347200</td>\n",
       "      <td>1.636500</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>947433600</td>\n",
       "      <td>1.637037</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>947520000</td>\n",
       "      <td>1.647868</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>947606400</td>\n",
       "      <td>1.647033</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>947692800</td>\n",
       "      <td>1.647693</td>\n",
       "      <td>-0.005462</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>947779200</td>\n",
       "      <td>1.636108</td>\n",
       "      <td>-0.007347</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>947865600</td>\n",
       "      <td>1.633000</td>\n",
       "      <td>-0.001554</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>947952000</td>\n",
       "      <td>1.633000</td>\n",
       "      <td>-0.001257</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>948038400</td>\n",
       "      <td>1.630486</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>948124800</td>\n",
       "      <td>1.637738</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>948211200</td>\n",
       "      <td>1.643449</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>948297600</td>\n",
       "      <td>1.654645</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>948384000</td>\n",
       "      <td>1.650359</td>\n",
       "      <td>-0.003523</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>948470400</td>\n",
       "      <td>1.647600</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>948556800</td>\n",
       "      <td>1.648500</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>948643200</td>\n",
       "      <td>1.652200</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>948729600</td>\n",
       "      <td>1.648148</td>\n",
       "      <td>-0.006698</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>948816000</td>\n",
       "      <td>1.638805</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>948902400</td>\n",
       "      <td>1.636098</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>948988800</td>\n",
       "      <td>1.620183</td>\n",
       "      <td>-0.007899</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>949075200</td>\n",
       "      <td>1.620300</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>949161600</td>\n",
       "      <td>1.620300</td>\n",
       "      <td>-0.001071</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6167</th>\n",
       "      <td>1480694400</td>\n",
       "      <td>1.272760</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6168</th>\n",
       "      <td>1480780800</td>\n",
       "      <td>1.272780</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6169</th>\n",
       "      <td>1480867200</td>\n",
       "      <td>1.270852</td>\n",
       "      <td>-0.001103</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6170</th>\n",
       "      <td>1480953600</td>\n",
       "      <td>1.270575</td>\n",
       "      <td>-0.005175</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>1481040000</td>\n",
       "      <td>1.260501</td>\n",
       "      <td>-0.006492</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6172</th>\n",
       "      <td>1481126400</td>\n",
       "      <td>1.257591</td>\n",
       "      <td>-0.001178</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6173</th>\n",
       "      <td>1481212800</td>\n",
       "      <td>1.258145</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>1481299200</td>\n",
       "      <td>1.257300</td>\n",
       "      <td>-0.000522</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6175</th>\n",
       "      <td>1481385600</td>\n",
       "      <td>1.257100</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6176</th>\n",
       "      <td>1481472000</td>\n",
       "      <td>1.267493</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6177</th>\n",
       "      <td>1481558400</td>\n",
       "      <td>1.268368</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178</th>\n",
       "      <td>1481644800</td>\n",
       "      <td>1.271379</td>\n",
       "      <td>-0.014918</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6179</th>\n",
       "      <td>1481731200</td>\n",
       "      <td>1.238533</td>\n",
       "      <td>-0.011847</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6180</th>\n",
       "      <td>1481817600</td>\n",
       "      <td>1.247686</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6181</th>\n",
       "      <td>1481904000</td>\n",
       "      <td>1.248950</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>1481990400</td>\n",
       "      <td>1.249000</td>\n",
       "      <td>-0.003711</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6183</th>\n",
       "      <td>1482076800</td>\n",
       "      <td>1.241528</td>\n",
       "      <td>-0.006678</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6184</th>\n",
       "      <td>1482163200</td>\n",
       "      <td>1.235645</td>\n",
       "      <td>-0.003242</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6185</th>\n",
       "      <td>1482249600</td>\n",
       "      <td>1.235043</td>\n",
       "      <td>-0.002854</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186</th>\n",
       "      <td>1482336000</td>\n",
       "      <td>1.229937</td>\n",
       "      <td>-0.004875</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6187</th>\n",
       "      <td>1482422400</td>\n",
       "      <td>1.225293</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6188</th>\n",
       "      <td>1482508800</td>\n",
       "      <td>1.228500</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6189</th>\n",
       "      <td>1482595200</td>\n",
       "      <td>1.227800</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>1482681600</td>\n",
       "      <td>1.228694</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6191</th>\n",
       "      <td>1482768000</td>\n",
       "      <td>1.226001</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6192</th>\n",
       "      <td>1482854400</td>\n",
       "      <td>1.222100</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6193</th>\n",
       "      <td>1482940800</td>\n",
       "      <td>1.223908</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6194</th>\n",
       "      <td>1483027200</td>\n",
       "      <td>1.233779</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6195</th>\n",
       "      <td>1483113600</td>\n",
       "      <td>1.233210</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6196</th>\n",
       "      <td>1483200000</td>\n",
       "      <td>1.233900</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6197 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp       gbp  gbp_gradient  gbp_gradient_binary  \\\n",
       "0      946656000  1.615700     -0.000700                    0   \n",
       "1      946742400  1.615000      0.005512                    1   \n",
       "2      946828800  1.626724      0.011001                    1   \n",
       "3      946915200  1.637002      0.005804                    1   \n",
       "4      947001600  1.638331      0.005379                    1   \n",
       "5      947088000  1.647760     -0.000018                    0   \n",
       "6      947174400  1.638294     -0.005630                    0   \n",
       "7      947260800  1.636500     -0.000897                    0   \n",
       "8      947347200  1.636500      0.000268                    1   \n",
       "9      947433600  1.637037      0.005684                    1   \n",
       "10     947520000  1.647868      0.004998                    1   \n",
       "11     947606400  1.647033     -0.000087                    0   \n",
       "12     947692800  1.647693     -0.005462                    0   \n",
       "13     947779200  1.636108     -0.007347                    0   \n",
       "14     947865600  1.633000     -0.001554                    0   \n",
       "15     947952000  1.633000     -0.001257                    0   \n",
       "16     948038400  1.630486      0.002369                    1   \n",
       "17     948124800  1.637738      0.006481                    1   \n",
       "18     948211200  1.643449      0.008453                    1   \n",
       "19     948297600  1.654645      0.003455                    1   \n",
       "20     948384000  1.650359     -0.003523                    0   \n",
       "21     948470400  1.647600     -0.000929                    0   \n",
       "22     948556800  1.648500      0.002300                    1   \n",
       "23     948643200  1.652200     -0.000176                    0   \n",
       "24     948729600  1.648148     -0.006698                    0   \n",
       "25     948816000  1.638805     -0.006025                    0   \n",
       "26     948902400  1.636098     -0.009311                    0   \n",
       "27     948988800  1.620183     -0.007899                    0   \n",
       "28     949075200  1.620300      0.000058                    1   \n",
       "29     949161600  1.620300     -0.001071                    0   \n",
       "...          ...       ...           ...                  ...   \n",
       "6167  1480694400  1.272760      0.002316                    1   \n",
       "6168  1480780800  1.272780     -0.000954                    0   \n",
       "6169  1480867200  1.270852     -0.001103                    0   \n",
       "6170  1480953600  1.270575     -0.005175                    0   \n",
       "6171  1481040000  1.260501     -0.006492                    0   \n",
       "6172  1481126400  1.257591     -0.001178                    0   \n",
       "6173  1481212800  1.258145     -0.000146                    0   \n",
       "6174  1481299200  1.257300     -0.000522                    0   \n",
       "6175  1481385600  1.257100      0.005096                    1   \n",
       "6176  1481472000  1.267493      0.005634                    1   \n",
       "6177  1481558400  1.268368      0.001943                    1   \n",
       "6178  1481644800  1.271379     -0.014918                    0   \n",
       "6179  1481731200  1.238533     -0.011847                    0   \n",
       "6180  1481817600  1.247686      0.005209                    1   \n",
       "6181  1481904000  1.248950      0.000657                    1   \n",
       "6182  1481990400  1.249000     -0.003711                    0   \n",
       "6183  1482076800  1.241528     -0.006678                    0   \n",
       "6184  1482163200  1.235645     -0.003242                    0   \n",
       "6185  1482249600  1.235043     -0.002854                    0   \n",
       "6186  1482336000  1.229937     -0.004875                    0   \n",
       "6187  1482422400  1.225293     -0.000719                    0   \n",
       "6188  1482508800  1.228500      0.001253                    1   \n",
       "6189  1482595200  1.227800      0.000097                    1   \n",
       "6190  1482681600  1.228694     -0.000900                    0   \n",
       "6191  1482768000  1.226001     -0.003297                    0   \n",
       "6192  1482854400  1.222100     -0.001046                    0   \n",
       "6193  1482940800  1.223908      0.005839                    1   \n",
       "6194  1483027200  1.233779      0.004651                    1   \n",
       "6195  1483113600  1.233210      0.000061                    1   \n",
       "6196  1483200000  1.233900      0.000690                    1   \n",
       "\n",
       "      gbp_gradient_binary_pred  \n",
       "0                          1.0  \n",
       "1                          1.0  \n",
       "2                          1.0  \n",
       "3                          1.0  \n",
       "4                          1.0  \n",
       "5                          0.0  \n",
       "6                          0.0  \n",
       "7                          0.0  \n",
       "8                          0.0  \n",
       "9                          1.0  \n",
       "10                         1.0  \n",
       "11                         1.0  \n",
       "12                         0.0  \n",
       "13                         1.0  \n",
       "14                         0.0  \n",
       "15                         1.0  \n",
       "16                         1.0  \n",
       "17                         1.0  \n",
       "18                         1.0  \n",
       "19                         1.0  \n",
       "20                         1.0  \n",
       "21                         0.0  \n",
       "22                         0.0  \n",
       "23                         1.0  \n",
       "24                         0.0  \n",
       "25                         1.0  \n",
       "26                         1.0  \n",
       "27                         0.0  \n",
       "28                         0.0  \n",
       "29                         1.0  \n",
       "...                        ...  \n",
       "6167                       1.0  \n",
       "6168                       1.0  \n",
       "6169                       0.0  \n",
       "6170                       0.0  \n",
       "6171                       1.0  \n",
       "6172                       1.0  \n",
       "6173                       0.0  \n",
       "6174                       0.0  \n",
       "6175                       0.0  \n",
       "6176                       1.0  \n",
       "6177                       1.0  \n",
       "6178                       1.0  \n",
       "6179                       0.0  \n",
       "6180                       0.0  \n",
       "6181                       1.0  \n",
       "6182                       1.0  \n",
       "6183                       1.0  \n",
       "6184                       0.0  \n",
       "6185                       0.0  \n",
       "6186                       0.0  \n",
       "6187                       1.0  \n",
       "6188                       1.0  \n",
       "6189                       1.0  \n",
       "6190                       0.0  \n",
       "6191                       0.0  \n",
       "6192                       1.0  \n",
       "6193                       1.0  \n",
       "6194                       1.0  \n",
       "6195                       1.0  \n",
       "6196                       1.0  \n",
       "\n",
       "[6197 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['gbp_gradient_binary_pred'] = binary_pred\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine = create_engine(\"mysql+mysqldb://news:\"+'newsnews'+\"@54.65.19.253/news\")\n",
    "#conn = MySQLdb.connect(host='54.65.19.253', port=3306, user='news', passwd='newsnews', db='news')\n",
    "#output.to_pickle('output.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7243 - acc: 0.5151\n",
      "Epoch 2/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7105 - acc: 0.5140\n",
      "Epoch 3/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7072 - acc: 0.5075\n",
      "Epoch 4/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7058 - acc: 0.5081\n",
      "Epoch 5/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7012 - acc: 0.5175\n",
      "Epoch 6/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7036 - acc: 0.5098\n",
      "Epoch 7/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7008 - acc: 0.5162\n",
      "Epoch 8/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6991 - acc: 0.5130\n",
      "Epoch 9/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6979 - acc: 0.5206\n",
      "Epoch 10/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6973 - acc: 0.5165\n",
      "Epoch 11/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6980 - acc: 0.5169\n",
      "Epoch 12/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6932 - acc: 0.5293\n",
      "Epoch 13/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6941 - acc: 0.5254\n",
      "Epoch 14/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6952 - acc: 0.5223\n",
      "Epoch 15/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6950 - acc: 0.5230\n",
      "Epoch 16/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6904 - acc: 0.5351\n",
      "Epoch 17/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6936 - acc: 0.5275\n",
      "Epoch 18/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6919 - acc: 0.5286\n",
      "Epoch 19/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6916 - acc: 0.5267\n",
      "Epoch 20/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6908 - acc: 0.5357\n",
      "Epoch 21/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6915 - acc: 0.5361\n",
      "Epoch 22/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6897 - acc: 0.5351\n",
      "Epoch 23/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6849 - acc: 0.5354\n",
      "Epoch 24/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6844 - acc: 0.5462\n",
      "Epoch 25/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6841 - acc: 0.5495\n",
      "Epoch 26/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6819 - acc: 0.5478\n",
      "Epoch 27/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6800 - acc: 0.5582\n",
      "Epoch 28/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6794 - acc: 0.5614\n",
      "Epoch 29/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6795 - acc: 0.5562\n",
      "Epoch 30/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6801 - acc: 0.5528\n",
      "Epoch 31/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6800 - acc: 0.5556\n",
      "Epoch 32/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6729 - acc: 0.5721\n",
      "Epoch 33/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6765 - acc: 0.5624\n",
      "Epoch 34/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6754 - acc: 0.5709\n",
      "Epoch 35/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6739 - acc: 0.5680\n",
      "Epoch 36/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6716 - acc: 0.5832\n",
      "Epoch 37/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6698 - acc: 0.5801\n",
      "Epoch 38/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6661 - acc: 0.5937\n",
      "Epoch 39/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6712 - acc: 0.5811\n",
      "Epoch 40/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6634 - acc: 0.5971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f91cdf99a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = create_model()\n",
    "# x = X.toarray()\n",
    "# y = np_utils.to_categorical(Y, num_classes)\n",
    "# model.fit(x, y, \n",
    "#         epochs=num_epochs,\n",
    "#         batch_size=7000,\n",
    "#         verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model.h5')\n",
    "# joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
