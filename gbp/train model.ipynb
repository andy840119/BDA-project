{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MySQLdb\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, StratifiedKFold, KFold\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Merge, Input, Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.externals import joblib\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary(Y):\n",
    "    Y[np.where(Y > 0)] = 1\n",
    "    Y[np.where(Y <= 0)] = 0\n",
    "    Y = Y.astype('int64')\n",
    "    f = np.bincount(Y)\n",
    "    print(f/np.sum(f))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shift(data, shift_offset=2):\n",
    "    g = data.as_matrix(columns=['gbp_gradient']).reshape(-1)\n",
    "    for i in range(1, shift_offset+1):\n",
    "        data['gbp_gradient_p_'+str(i)] = shift(g, i, cval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('newstitle_gbp(uk-news,world,business,technology,money_2000-2016).csv')\n",
    "data['gbp_gradient'] = np.gradient(data.as_matrix(columns=['gbp']).reshape(-1))\n",
    "get_shift(data, shift_offset=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>gbp</th>\n",
       "      <th>gbp_gradient</th>\n",
       "      <th>gbp_gradient_p_1</th>\n",
       "      <th>gbp_gradient_p_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946656000</td>\n",
       "      <td>Big Macs, small horizons Y2K force outstrips s...</td>\n",
       "      <td>1.615700</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>946742400</td>\n",
       "      <td>Cock-fighting? Yes please Samaritans split ove...</td>\n",
       "      <td>1.615000</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>946828800</td>\n",
       "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
       "      <td>1.626724</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>-0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>946915200</td>\n",
       "      <td>Leader: German sleaze inquiry Burundi peace in...</td>\n",
       "      <td>1.637002</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.005512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>947001600</td>\n",
       "      <td>Women in record South Pole walk Deaths no coin...</td>\n",
       "      <td>1.638331</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.011001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                                              title       gbp  \\\n",
       "0  946656000  Big Macs, small horizons Y2K force outstrips s...  1.615700   \n",
       "1  946742400  Cock-fighting? Yes please Samaritans split ove...  1.615000   \n",
       "2  946828800  Hunters threaten Jospin with new battle of the...  1.626724   \n",
       "3  946915200  Leader: German sleaze inquiry Burundi peace in...  1.637002   \n",
       "4  947001600  Women in record South Pole walk Deaths no coin...  1.638331   \n",
       "\n",
       "   gbp_gradient  gbp_gradient_p_1  gbp_gradient_p_2  \n",
       "0     -0.000700          0.000000          0.000000  \n",
       "1      0.005512         -0.000700          0.000000  \n",
       "2      0.011001          0.005512         -0.000700  \n",
       "3      0.005804          0.011001          0.005512  \n",
       "4      0.005379          0.005804          0.011001  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.47942553  0.52057447]\n"
     ]
    }
   ],
   "source": [
    "Y = data.as_matrix(columns=['gbp_gradient']).reshape(-1)\n",
    "Y = shift(Y, -1, cval=3)\n",
    "Y = binary(Y)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.47974827  0.52025173]\n",
      "[ 0.47990963  0.52009037]\n"
     ]
    }
   ],
   "source": [
    "X_p = data.as_matrix(columns=['gbp_gradient_p_1', 'gbp_gradient_p_2'])\n",
    "X_p[:,0] = binary(X_p[:,0])\n",
    "X_p[:,1] = binary(X_p[:,1])\n",
    "# X_p = data.as_matrix(columns=['gbp_gradient','gbp_gradient_p_1'])\n",
    "# X_p[:,0] = binary(X_p[:,0])\n",
    "# X_p[:,1] = binary(X_p[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6197x2730 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 476823 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_name = 'title'\n",
    "vectorizer = TfidfVectorizer(min_df=50, ngram_range=(1, 4))\n",
    "X = vectorizer.fit_transform(data[feature_name].tolist())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with_news = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if with_news:\n",
    "    X = hstack([X, X.power(2), X.power(3), X.power(4), X_p])\n",
    "else:\n",
    "    X = X_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='tanh', input_shape=(X.shape[1],), bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='tanh', bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='tanh', bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='tanh', bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='RMSprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output = pd.DataFrame()\n",
    "# output['timestamp'] = data['timestamp']\n",
    "# output['gbp'] = data['gbp']\n",
    "# output['gbp_gradient'] = data['gbp_gradient']\n",
    "# output['gbp_gradient_binary'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_result(X_test, y_test, binary_pred, test_idx):\n",
    "    target_names = ['decline','up']\n",
    "    pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    binary_pred[test_idx] = pred\n",
    "    #print(confusion_matrix(y_test, pred, labels=[0,1]))\n",
    "    mat = confusion_matrix(y_test, pred, labels=[0,1])\n",
    "    report = classification_report(y_test, pred, target_names=target_names) + '\\n'\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4956 samples, validate on 1241 samples\n",
      "Epoch 1/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.7116 - acc: 0.4968 - val_loss: 0.6922 - val_acc: 0.5342\n",
      "Epoch 2/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.7045 - acc: 0.5044 - val_loss: 0.6914 - val_acc: 0.5270\n",
      "Epoch 3/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6994 - acc: 0.5075 - val_loss: 0.6913 - val_acc: 0.5278\n",
      "Epoch 4/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6943 - acc: 0.5141 - val_loss: 0.6915 - val_acc: 0.5463\n",
      "Epoch 5/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6898 - acc: 0.5287 - val_loss: 0.6914 - val_acc: 0.5230\n",
      "Epoch 6/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6910 - acc: 0.5331 - val_loss: 0.6916 - val_acc: 0.5399\n",
      "Epoch 7/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6879 - acc: 0.5402 - val_loss: 0.6917 - val_acc: 0.5391\n",
      "Epoch 8/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6861 - acc: 0.5400 - val_loss: 0.6914 - val_acc: 0.5391\n",
      "Epoch 9/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6860 - acc: 0.5389 - val_loss: 0.6914 - val_acc: 0.5375\n",
      "Epoch 10/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6821 - acc: 0.5527 - val_loss: 0.6915 - val_acc: 0.5439\n",
      "Epoch 11/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6820 - acc: 0.5527 - val_loss: 0.6913 - val_acc: 0.5334\n",
      "Epoch 12/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6805 - acc: 0.5583 - val_loss: 0.6914 - val_acc: 0.5471\n",
      "Epoch 13/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6786 - acc: 0.5658 - val_loss: 0.6913 - val_acc: 0.5334\n",
      "Epoch 14/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6759 - acc: 0.5704 - val_loss: 0.6914 - val_acc: 0.5334\n",
      "Epoch 15/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6760 - acc: 0.5769 - val_loss: 0.6916 - val_acc: 0.5310\n",
      "Epoch 16/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6750 - acc: 0.5734 - val_loss: 0.6918 - val_acc: 0.5318\n",
      "Epoch 17/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6734 - acc: 0.5841 - val_loss: 0.6919 - val_acc: 0.5351\n",
      "Epoch 18/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6711 - acc: 0.5825 - val_loss: 0.6920 - val_acc: 0.5334\n",
      "Epoch 19/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6673 - acc: 0.5940 - val_loss: 0.6921 - val_acc: 0.5286\n",
      "Epoch 20/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6691 - acc: 0.5849 - val_loss: 0.6927 - val_acc: 0.5133\n",
      "Epoch 21/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6644 - acc: 0.6156 - val_loss: 0.6929 - val_acc: 0.5181\n",
      "Epoch 22/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6615 - acc: 0.6084 - val_loss: 0.6931 - val_acc: 0.5189\n",
      "Epoch 23/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6609 - acc: 0.6154 - val_loss: 0.6935 - val_acc: 0.5197\n",
      "Epoch 24/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6609 - acc: 0.6182 - val_loss: 0.6938 - val_acc: 0.5214\n",
      "Epoch 25/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6566 - acc: 0.6211 - val_loss: 0.6949 - val_acc: 0.5060\n",
      "Epoch 26/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6518 - acc: 0.6412 - val_loss: 0.6942 - val_acc: 0.5254\n",
      "Epoch 27/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6528 - acc: 0.6334 - val_loss: 0.6951 - val_acc: 0.5149\n",
      "Epoch 28/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6513 - acc: 0.6394 - val_loss: 0.6952 - val_acc: 0.5165\n",
      "Epoch 29/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6479 - acc: 0.6445 - val_loss: 0.6957 - val_acc: 0.5189\n",
      "Epoch 30/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6474 - acc: 0.6437 - val_loss: 0.6962 - val_acc: 0.5157\n",
      "Epoch 31/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6440 - acc: 0.6608 - val_loss: 0.6978 - val_acc: 0.5052\n",
      "Epoch 32/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6421 - acc: 0.6564 - val_loss: 0.6981 - val_acc: 0.5077\n",
      "Epoch 33/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6397 - acc: 0.6507 - val_loss: 0.6994 - val_acc: 0.5052\n",
      "Epoch 34/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6336 - acc: 0.6762 - val_loss: 0.6990 - val_acc: 0.5214\n",
      "Epoch 35/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6334 - acc: 0.6725 - val_loss: 0.7010 - val_acc: 0.5028\n",
      "Epoch 36/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6324 - acc: 0.6727 - val_loss: 0.7012 - val_acc: 0.5141\n",
      "Epoch 37/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6299 - acc: 0.6580 - val_loss: 0.7020 - val_acc: 0.5181\n",
      "Epoch 38/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6238 - acc: 0.6796 - val_loss: 0.7027 - val_acc: 0.5238\n",
      "Epoch 39/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6200 - acc: 0.6935 - val_loss: 0.7034 - val_acc: 0.5214\n",
      "Epoch 40/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6165 - acc: 0.6933 - val_loss: 0.7045 - val_acc: 0.5205\n",
      "Epoch 41/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6148 - acc: 0.6949 - val_loss: 0.7063 - val_acc: 0.5157\n",
      "Epoch 42/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6188 - acc: 0.6824 - val_loss: 0.7067 - val_acc: 0.5222\n",
      "Epoch 43/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6164 - acc: 0.6881 - val_loss: 0.7089 - val_acc: 0.5077\n",
      "Epoch 44/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6084 - acc: 0.7028 - val_loss: 0.7102 - val_acc: 0.5173\n",
      "Epoch 45/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6128 - acc: 0.6935 - val_loss: 0.7124 - val_acc: 0.5068\n",
      "Epoch 46/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6112 - acc: 0.7050 - val_loss: 0.7134 - val_acc: 0.5133\n",
      "Epoch 47/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5986 - acc: 0.6994 - val_loss: 0.7157 - val_acc: 0.4956\n",
      "Epoch 48/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6009 - acc: 0.7151 - val_loss: 0.7174 - val_acc: 0.5117\n",
      "Epoch 49/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5993 - acc: 0.7012 - val_loss: 0.7222 - val_acc: 0.4907\n",
      "Epoch 50/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.6001 - acc: 0.7125 - val_loss: 0.7204 - val_acc: 0.5101\n",
      "Epoch 51/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5966 - acc: 0.7058 - val_loss: 0.7230 - val_acc: 0.4956\n",
      "Epoch 52/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5951 - acc: 0.7211 - val_loss: 0.7238 - val_acc: 0.5133\n",
      "Epoch 53/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4956/4956 [==============================] - 0s - loss: 0.5945 - acc: 0.7123 - val_loss: 0.7245 - val_acc: 0.5125\n",
      "Epoch 54/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5889 - acc: 0.7272 - val_loss: 0.7264 - val_acc: 0.5222\n",
      "Epoch 55/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5807 - acc: 0.7296 - val_loss: 0.7283 - val_acc: 0.5165\n",
      "Epoch 56/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5832 - acc: 0.7260 - val_loss: 0.7302 - val_acc: 0.5222\n",
      "Epoch 57/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5817 - acc: 0.7238 - val_loss: 0.7327 - val_acc: 0.5133\n",
      "Epoch 58/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5745 - acc: 0.7359 - val_loss: 0.7342 - val_acc: 0.5189\n",
      "Epoch 59/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5727 - acc: 0.7389 - val_loss: 0.7363 - val_acc: 0.5197\n",
      "Epoch 60/60\n",
      "4956/4956 [==============================] - 0s - loss: 0.5705 - acc: 0.7460 - val_loss: 0.7387 - val_acc: 0.5197\n",
      "1241/1241 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7079 - acc: 0.4948 - val_loss: 0.6929 - val_acc: 0.5125\n",
      "Epoch 2/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7041 - acc: 0.4901 - val_loss: 0.6927 - val_acc: 0.5125\n",
      "Epoch 3/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7003 - acc: 0.4980 - val_loss: 0.6926 - val_acc: 0.5246\n",
      "Epoch 4/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6953 - acc: 0.5107 - val_loss: 0.6925 - val_acc: 0.5214\n",
      "Epoch 5/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6933 - acc: 0.5192 - val_loss: 0.6925 - val_acc: 0.5214\n",
      "Epoch 6/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6955 - acc: 0.5095 - val_loss: 0.6925 - val_acc: 0.5206\n",
      "Epoch 7/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6923 - acc: 0.5242 - val_loss: 0.6926 - val_acc: 0.5206\n",
      "Epoch 8/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6950 - acc: 0.5175 - val_loss: 0.6926 - val_acc: 0.5214\n",
      "Epoch 9/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6909 - acc: 0.5222 - val_loss: 0.6926 - val_acc: 0.5222\n",
      "Epoch 10/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6907 - acc: 0.5321 - val_loss: 0.6926 - val_acc: 0.5182\n",
      "Epoch 11/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6934 - acc: 0.5131 - val_loss: 0.6926 - val_acc: 0.5238\n",
      "Epoch 12/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6915 - acc: 0.5238 - val_loss: 0.6927 - val_acc: 0.5157\n",
      "Epoch 13/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6907 - acc: 0.5246 - val_loss: 0.6927 - val_acc: 0.5117\n",
      "Epoch 14/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6911 - acc: 0.5202 - val_loss: 0.6926 - val_acc: 0.5206\n",
      "Epoch 15/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6897 - acc: 0.5375 - val_loss: 0.6926 - val_acc: 0.5141\n",
      "Epoch 16/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6899 - acc: 0.5238 - val_loss: 0.6927 - val_acc: 0.5133\n",
      "Epoch 17/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6900 - acc: 0.5301 - val_loss: 0.6927 - val_acc: 0.5141\n",
      "Epoch 18/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6902 - acc: 0.5331 - val_loss: 0.6927 - val_acc: 0.5165\n",
      "Epoch 19/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6879 - acc: 0.5379 - val_loss: 0.6928 - val_acc: 0.5157\n",
      "Epoch 20/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6878 - acc: 0.5359 - val_loss: 0.6928 - val_acc: 0.5157\n",
      "Epoch 21/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6879 - acc: 0.5381 - val_loss: 0.6929 - val_acc: 0.5141\n",
      "Epoch 22/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6866 - acc: 0.5375 - val_loss: 0.6930 - val_acc: 0.5125\n",
      "Epoch 23/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6829 - acc: 0.5476 - val_loss: 0.6932 - val_acc: 0.5141\n",
      "Epoch 24/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6829 - acc: 0.5432 - val_loss: 0.6933 - val_acc: 0.5149\n",
      "Epoch 25/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6829 - acc: 0.5486 - val_loss: 0.6933 - val_acc: 0.5198\n",
      "Epoch 26/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6805 - acc: 0.5676 - val_loss: 0.6934 - val_acc: 0.5077\n",
      "Epoch 27/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6778 - acc: 0.5625 - val_loss: 0.6936 - val_acc: 0.5198\n",
      "Epoch 28/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6799 - acc: 0.5625 - val_loss: 0.6937 - val_acc: 0.5028\n",
      "Epoch 29/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6772 - acc: 0.5797 - val_loss: 0.6939 - val_acc: 0.5165\n",
      "Epoch 30/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6784 - acc: 0.5672 - val_loss: 0.6941 - val_acc: 0.5028\n",
      "Epoch 31/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6759 - acc: 0.5619 - val_loss: 0.6942 - val_acc: 0.4996\n",
      "Epoch 32/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6745 - acc: 0.5821 - val_loss: 0.6943 - val_acc: 0.5028\n",
      "Epoch 33/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6722 - acc: 0.5877 - val_loss: 0.6944 - val_acc: 0.5052\n",
      "Epoch 34/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6735 - acc: 0.5817 - val_loss: 0.6946 - val_acc: 0.5036\n",
      "Epoch 35/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6706 - acc: 0.5998 - val_loss: 0.6948 - val_acc: 0.5028\n",
      "Epoch 36/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6695 - acc: 0.5938 - val_loss: 0.6950 - val_acc: 0.4988\n",
      "Epoch 37/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6678 - acc: 0.5992 - val_loss: 0.6952 - val_acc: 0.5028\n",
      "Epoch 38/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6675 - acc: 0.5948 - val_loss: 0.6955 - val_acc: 0.5028\n",
      "Epoch 39/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6658 - acc: 0.5936 - val_loss: 0.6958 - val_acc: 0.4899\n",
      "Epoch 40/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6618 - acc: 0.6069 - val_loss: 0.6962 - val_acc: 0.4972\n",
      "Epoch 41/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6615 - acc: 0.6075 - val_loss: 0.6963 - val_acc: 0.4972\n",
      "Epoch 42/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6598 - acc: 0.6186 - val_loss: 0.6969 - val_acc: 0.4939\n",
      "Epoch 43/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6585 - acc: 0.6156 - val_loss: 0.6969 - val_acc: 0.4948\n",
      "Epoch 44/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6574 - acc: 0.6214 - val_loss: 0.6977 - val_acc: 0.4875\n",
      "Epoch 45/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4958/4958 [==============================] - 0s - loss: 0.6562 - acc: 0.6263 - val_loss: 0.6977 - val_acc: 0.4907\n",
      "Epoch 46/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6505 - acc: 0.6277 - val_loss: 0.6981 - val_acc: 0.4980\n",
      "Epoch 47/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6540 - acc: 0.6277 - val_loss: 0.6984 - val_acc: 0.4956\n",
      "Epoch 48/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6510 - acc: 0.6319 - val_loss: 0.6989 - val_acc: 0.4996\n",
      "Epoch 49/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6499 - acc: 0.6315 - val_loss: 0.6994 - val_acc: 0.4956\n",
      "Epoch 50/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6451 - acc: 0.6501 - val_loss: 0.6999 - val_acc: 0.4923\n",
      "Epoch 51/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6460 - acc: 0.6341 - val_loss: 0.7005 - val_acc: 0.4956\n",
      "Epoch 52/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6446 - acc: 0.6428 - val_loss: 0.7012 - val_acc: 0.4931\n",
      "Epoch 53/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6399 - acc: 0.6410 - val_loss: 0.7020 - val_acc: 0.4948\n",
      "Epoch 54/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6369 - acc: 0.6545 - val_loss: 0.7027 - val_acc: 0.4923\n",
      "Epoch 55/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6354 - acc: 0.6620 - val_loss: 0.7035 - val_acc: 0.4899\n",
      "Epoch 56/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6363 - acc: 0.6521 - val_loss: 0.7045 - val_acc: 0.4899\n",
      "Epoch 57/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6348 - acc: 0.6583 - val_loss: 0.7058 - val_acc: 0.4778\n",
      "Epoch 58/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6301 - acc: 0.6603 - val_loss: 0.7063 - val_acc: 0.4851\n",
      "Epoch 59/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6277 - acc: 0.6737 - val_loss: 0.7071 - val_acc: 0.4843\n",
      "Epoch 60/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6276 - acc: 0.6684 - val_loss: 0.7091 - val_acc: 0.4818\n",
      "1239/1239 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7123 - acc: 0.4794 - val_loss: 0.6926 - val_acc: 0.5190\n",
      "Epoch 2/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7036 - acc: 0.4948 - val_loss: 0.6924 - val_acc: 0.5198\n",
      "Epoch 3/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7000 - acc: 0.5006 - val_loss: 0.6924 - val_acc: 0.5190\n",
      "Epoch 4/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6983 - acc: 0.5077 - val_loss: 0.6925 - val_acc: 0.5190\n",
      "Epoch 5/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6948 - acc: 0.5188 - val_loss: 0.6925 - val_acc: 0.5198\n",
      "Epoch 6/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6946 - acc: 0.5186 - val_loss: 0.6925 - val_acc: 0.5198\n",
      "Epoch 7/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6939 - acc: 0.5190 - val_loss: 0.6925 - val_acc: 0.5198\n",
      "Epoch 8/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6945 - acc: 0.5254 - val_loss: 0.6924 - val_acc: 0.5198\n",
      "Epoch 9/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6945 - acc: 0.5129 - val_loss: 0.6925 - val_acc: 0.5198\n",
      "Epoch 10/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6924 - acc: 0.5270 - val_loss: 0.6925 - val_acc: 0.5190\n",
      "Epoch 11/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6930 - acc: 0.5184 - val_loss: 0.6925 - val_acc: 0.5190\n",
      "Epoch 12/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6914 - acc: 0.5262 - val_loss: 0.6926 - val_acc: 0.5214\n",
      "Epoch 13/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6920 - acc: 0.5242 - val_loss: 0.6926 - val_acc: 0.5206\n",
      "Epoch 14/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6925 - acc: 0.5276 - val_loss: 0.6925 - val_acc: 0.5190\n",
      "Epoch 15/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6907 - acc: 0.5202 - val_loss: 0.6926 - val_acc: 0.5190\n",
      "Epoch 16/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6904 - acc: 0.5220 - val_loss: 0.6926 - val_acc: 0.5230\n",
      "Epoch 17/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6906 - acc: 0.5246 - val_loss: 0.6926 - val_acc: 0.5182\n",
      "Epoch 18/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6876 - acc: 0.5337 - val_loss: 0.6927 - val_acc: 0.5214\n",
      "Epoch 19/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6906 - acc: 0.5238 - val_loss: 0.6927 - val_acc: 0.5190\n",
      "Epoch 20/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6893 - acc: 0.5303 - val_loss: 0.6927 - val_acc: 0.5149\n",
      "Epoch 21/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6874 - acc: 0.5319 - val_loss: 0.6927 - val_acc: 0.5141\n",
      "Epoch 22/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6859 - acc: 0.5520 - val_loss: 0.6927 - val_acc: 0.5165\n",
      "Epoch 23/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6851 - acc: 0.5411 - val_loss: 0.6928 - val_acc: 0.5157\n",
      "Epoch 24/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6834 - acc: 0.5599 - val_loss: 0.6929 - val_acc: 0.5149\n",
      "Epoch 25/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6838 - acc: 0.5518 - val_loss: 0.6930 - val_acc: 0.5117\n",
      "Epoch 26/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6834 - acc: 0.5555 - val_loss: 0.6930 - val_acc: 0.5077\n",
      "Epoch 27/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6817 - acc: 0.5619 - val_loss: 0.6930 - val_acc: 0.5101\n",
      "Epoch 28/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6809 - acc: 0.5577 - val_loss: 0.6930 - val_acc: 0.5133\n",
      "Epoch 29/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6786 - acc: 0.5688 - val_loss: 0.6931 - val_acc: 0.5069\n",
      "Epoch 30/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6758 - acc: 0.5738 - val_loss: 0.6931 - val_acc: 0.5069\n",
      "Epoch 31/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6760 - acc: 0.5720 - val_loss: 0.6932 - val_acc: 0.5101\n",
      "Epoch 32/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6779 - acc: 0.5718 - val_loss: 0.6933 - val_acc: 0.5109\n",
      "Epoch 33/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6751 - acc: 0.5734 - val_loss: 0.6934 - val_acc: 0.5020\n",
      "Epoch 34/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6738 - acc: 0.5795 - val_loss: 0.6936 - val_acc: 0.5069\n",
      "Epoch 35/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6750 - acc: 0.5658 - val_loss: 0.6936 - val_acc: 0.5125\n",
      "Epoch 36/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6716 - acc: 0.5889 - val_loss: 0.6937 - val_acc: 0.5117\n",
      "Epoch 37/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4958/4958 [==============================] - 0s - loss: 0.6698 - acc: 0.5871 - val_loss: 0.6938 - val_acc: 0.5125\n",
      "Epoch 38/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6677 - acc: 0.5954 - val_loss: 0.6941 - val_acc: 0.5157\n",
      "Epoch 39/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6672 - acc: 0.5926 - val_loss: 0.6943 - val_acc: 0.5125\n",
      "Epoch 40/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6651 - acc: 0.5948 - val_loss: 0.6944 - val_acc: 0.5028\n",
      "Epoch 41/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6637 - acc: 0.6067 - val_loss: 0.6945 - val_acc: 0.5020\n",
      "Epoch 42/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6653 - acc: 0.6045 - val_loss: 0.6947 - val_acc: 0.5052\n",
      "Epoch 43/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6583 - acc: 0.6095 - val_loss: 0.6950 - val_acc: 0.5117\n",
      "Epoch 44/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6614 - acc: 0.5982 - val_loss: 0.6952 - val_acc: 0.4996\n",
      "Epoch 45/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6586 - acc: 0.6132 - val_loss: 0.6953 - val_acc: 0.5052\n",
      "Epoch 46/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6557 - acc: 0.6117 - val_loss: 0.6956 - val_acc: 0.5085\n",
      "Epoch 47/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6542 - acc: 0.6255 - val_loss: 0.6959 - val_acc: 0.5028\n",
      "Epoch 48/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6507 - acc: 0.6210 - val_loss: 0.6965 - val_acc: 0.5117\n",
      "Epoch 49/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6498 - acc: 0.6279 - val_loss: 0.6969 - val_acc: 0.5044\n",
      "Epoch 50/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6451 - acc: 0.6382 - val_loss: 0.6973 - val_acc: 0.5028\n",
      "Epoch 51/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6474 - acc: 0.6400 - val_loss: 0.6978 - val_acc: 0.4939\n",
      "Epoch 52/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6442 - acc: 0.6466 - val_loss: 0.6981 - val_acc: 0.5044\n",
      "Epoch 53/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6418 - acc: 0.6400 - val_loss: 0.6986 - val_acc: 0.5101\n",
      "Epoch 54/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6369 - acc: 0.6575 - val_loss: 0.6990 - val_acc: 0.4980\n",
      "Epoch 55/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6390 - acc: 0.6460 - val_loss: 0.6994 - val_acc: 0.5028\n",
      "Epoch 56/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6360 - acc: 0.6561 - val_loss: 0.7004 - val_acc: 0.4996\n",
      "Epoch 57/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6369 - acc: 0.6529 - val_loss: 0.7012 - val_acc: 0.4907\n",
      "Epoch 58/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6336 - acc: 0.6589 - val_loss: 0.7022 - val_acc: 0.5052\n",
      "Epoch 59/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6316 - acc: 0.6531 - val_loss: 0.7024 - val_acc: 0.4980\n",
      "Epoch 60/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6266 - acc: 0.6704 - val_loss: 0.7034 - val_acc: 0.5093\n",
      "1239/1239 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7114 - acc: 0.4978 - val_loss: 0.6935 - val_acc: 0.5052\n",
      "Epoch 2/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7082 - acc: 0.4857 - val_loss: 0.6932 - val_acc: 0.5182\n",
      "Epoch 3/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6997 - acc: 0.5131 - val_loss: 0.6932 - val_acc: 0.5190\n",
      "Epoch 4/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6967 - acc: 0.5117 - val_loss: 0.6931 - val_acc: 0.5206\n",
      "Epoch 5/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6981 - acc: 0.5089 - val_loss: 0.6931 - val_acc: 0.5198\n",
      "Epoch 6/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6954 - acc: 0.5139 - val_loss: 0.6931 - val_acc: 0.5198\n",
      "Epoch 7/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6950 - acc: 0.5196 - val_loss: 0.6931 - val_acc: 0.5198\n",
      "Epoch 8/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6943 - acc: 0.5165 - val_loss: 0.6932 - val_acc: 0.5198\n",
      "Epoch 9/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6949 - acc: 0.5236 - val_loss: 0.6931 - val_acc: 0.5206\n",
      "Epoch 10/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6940 - acc: 0.5171 - val_loss: 0.6931 - val_acc: 0.5206\n",
      "Epoch 11/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6953 - acc: 0.5141 - val_loss: 0.6931 - val_acc: 0.5198\n",
      "Epoch 12/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6933 - acc: 0.5175 - val_loss: 0.6932 - val_acc: 0.5214\n",
      "Epoch 13/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6917 - acc: 0.5153 - val_loss: 0.6932 - val_acc: 0.5206\n",
      "Epoch 14/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6911 - acc: 0.5286 - val_loss: 0.6932 - val_acc: 0.5214\n",
      "Epoch 15/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6906 - acc: 0.5258 - val_loss: 0.6933 - val_acc: 0.5222\n",
      "Epoch 16/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6907 - acc: 0.5292 - val_loss: 0.6934 - val_acc: 0.5174\n",
      "Epoch 17/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6907 - acc: 0.5228 - val_loss: 0.6933 - val_acc: 0.5206\n",
      "Epoch 18/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6894 - acc: 0.5266 - val_loss: 0.6934 - val_acc: 0.5206\n",
      "Epoch 19/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6880 - acc: 0.5377 - val_loss: 0.6934 - val_acc: 0.5149\n",
      "Epoch 20/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6893 - acc: 0.5337 - val_loss: 0.6936 - val_acc: 0.5198\n",
      "Epoch 21/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6855 - acc: 0.5409 - val_loss: 0.6937 - val_acc: 0.5190\n",
      "Epoch 22/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6861 - acc: 0.5301 - val_loss: 0.6936 - val_acc: 0.5198\n",
      "Epoch 23/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6859 - acc: 0.5395 - val_loss: 0.6936 - val_acc: 0.5149\n",
      "Epoch 24/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6865 - acc: 0.5393 - val_loss: 0.6937 - val_acc: 0.5157\n",
      "Epoch 25/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6825 - acc: 0.5547 - val_loss: 0.6938 - val_acc: 0.5190\n",
      "Epoch 26/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6829 - acc: 0.5591 - val_loss: 0.6938 - val_acc: 0.5222\n",
      "Epoch 27/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6814 - acc: 0.5549 - val_loss: 0.6940 - val_acc: 0.5141\n",
      "Epoch 28/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6810 - acc: 0.5474 - val_loss: 0.6940 - val_acc: 0.5182\n",
      "Epoch 29/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4958/4958 [==============================] - 0s - loss: 0.6800 - acc: 0.5593 - val_loss: 0.6946 - val_acc: 0.5117\n",
      "Epoch 30/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6787 - acc: 0.5496 - val_loss: 0.6944 - val_acc: 0.5109\n",
      "Epoch 31/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6766 - acc: 0.5581 - val_loss: 0.6946 - val_acc: 0.5061\n",
      "Epoch 32/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6770 - acc: 0.5569 - val_loss: 0.6947 - val_acc: 0.5036\n",
      "Epoch 33/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6753 - acc: 0.5643 - val_loss: 0.6951 - val_acc: 0.5141\n",
      "Epoch 34/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6749 - acc: 0.5712 - val_loss: 0.6950 - val_acc: 0.5028\n",
      "Epoch 35/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6710 - acc: 0.5805 - val_loss: 0.6953 - val_acc: 0.5052\n",
      "Epoch 36/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6726 - acc: 0.5777 - val_loss: 0.6955 - val_acc: 0.4931\n",
      "Epoch 37/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6704 - acc: 0.5801 - val_loss: 0.6956 - val_acc: 0.5052\n",
      "Epoch 38/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6708 - acc: 0.5740 - val_loss: 0.6960 - val_acc: 0.4923\n",
      "Epoch 39/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6683 - acc: 0.5869 - val_loss: 0.6965 - val_acc: 0.5101\n",
      "Epoch 40/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6645 - acc: 0.5867 - val_loss: 0.6964 - val_acc: 0.4996\n",
      "Epoch 41/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6645 - acc: 0.5984 - val_loss: 0.6966 - val_acc: 0.4988\n",
      "Epoch 42/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6635 - acc: 0.5958 - val_loss: 0.6969 - val_acc: 0.5036\n",
      "Epoch 43/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6617 - acc: 0.6008 - val_loss: 0.6971 - val_acc: 0.4996\n",
      "Epoch 44/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6616 - acc: 0.6047 - val_loss: 0.6975 - val_acc: 0.5052\n",
      "Epoch 45/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6576 - acc: 0.6115 - val_loss: 0.6978 - val_acc: 0.5061\n",
      "Epoch 46/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6592 - acc: 0.6002 - val_loss: 0.6985 - val_acc: 0.5061\n",
      "Epoch 47/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6561 - acc: 0.6142 - val_loss: 0.6987 - val_acc: 0.5020\n",
      "Epoch 48/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6574 - acc: 0.6027 - val_loss: 0.6993 - val_acc: 0.5069\n",
      "Epoch 49/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6533 - acc: 0.6103 - val_loss: 0.6998 - val_acc: 0.4988\n",
      "Epoch 50/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6534 - acc: 0.6204 - val_loss: 0.7010 - val_acc: 0.5028\n",
      "Epoch 51/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6500 - acc: 0.6154 - val_loss: 0.7008 - val_acc: 0.4972\n",
      "Epoch 52/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6472 - acc: 0.6206 - val_loss: 0.7012 - val_acc: 0.5036\n",
      "Epoch 53/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6483 - acc: 0.6230 - val_loss: 0.7022 - val_acc: 0.5020\n",
      "Epoch 54/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6442 - acc: 0.6347 - val_loss: 0.7025 - val_acc: 0.4956\n",
      "Epoch 55/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6442 - acc: 0.6392 - val_loss: 0.7030 - val_acc: 0.4996\n",
      "Epoch 56/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6413 - acc: 0.6380 - val_loss: 0.7032 - val_acc: 0.5061\n",
      "Epoch 57/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6393 - acc: 0.6396 - val_loss: 0.7040 - val_acc: 0.5020\n",
      "Epoch 58/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6356 - acc: 0.6549 - val_loss: 0.7050 - val_acc: 0.5101\n",
      "Epoch 59/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6344 - acc: 0.6343 - val_loss: 0.7054 - val_acc: 0.5036\n",
      "Epoch 60/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6306 - acc: 0.6497 - val_loss: 0.7062 - val_acc: 0.5012\n",
      "1239/1239 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7671 - acc: 0.4984 - val_loss: 0.6939 - val_acc: 0.5149\n",
      "Epoch 2/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7361 - acc: 0.5020 - val_loss: 0.6940 - val_acc: 0.5206\n",
      "Epoch 3/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7281 - acc: 0.5002 - val_loss: 0.6936 - val_acc: 0.5093\n",
      "Epoch 4/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7215 - acc: 0.5040 - val_loss: 0.6933 - val_acc: 0.5085\n",
      "Epoch 5/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7114 - acc: 0.5081 - val_loss: 0.6932 - val_acc: 0.5004\n",
      "Epoch 6/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7094 - acc: 0.5067 - val_loss: 0.6933 - val_acc: 0.5069\n",
      "Epoch 7/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7056 - acc: 0.5056 - val_loss: 0.6930 - val_acc: 0.5012\n",
      "Epoch 8/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7055 - acc: 0.5169 - val_loss: 0.6930 - val_acc: 0.4988\n",
      "Epoch 9/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7026 - acc: 0.5081 - val_loss: 0.6930 - val_acc: 0.4948\n",
      "Epoch 10/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.7013 - acc: 0.5147 - val_loss: 0.6933 - val_acc: 0.5004\n",
      "Epoch 11/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6949 - acc: 0.5288 - val_loss: 0.6930 - val_acc: 0.5069\n",
      "Epoch 12/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6990 - acc: 0.5117 - val_loss: 0.6930 - val_acc: 0.5036\n",
      "Epoch 13/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6925 - acc: 0.5325 - val_loss: 0.6928 - val_acc: 0.5077\n",
      "Epoch 14/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6933 - acc: 0.5276 - val_loss: 0.6925 - val_acc: 0.5036\n",
      "Epoch 15/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6953 - acc: 0.5196 - val_loss: 0.6927 - val_acc: 0.5093\n",
      "Epoch 16/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6919 - acc: 0.5321 - val_loss: 0.6926 - val_acc: 0.5085\n",
      "Epoch 17/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6913 - acc: 0.5260 - val_loss: 0.6931 - val_acc: 0.5028\n",
      "Epoch 18/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6884 - acc: 0.5286 - val_loss: 0.6929 - val_acc: 0.5044\n",
      "Epoch 19/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6844 - acc: 0.5450 - val_loss: 0.6929 - val_acc: 0.5085\n",
      "Epoch 20/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6836 - acc: 0.5492 - val_loss: 0.6933 - val_acc: 0.5077\n",
      "Epoch 21/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4958/4958 [==============================] - 0s - loss: 0.6831 - acc: 0.5430 - val_loss: 0.6931 - val_acc: 0.5085\n",
      "Epoch 22/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6822 - acc: 0.5470 - val_loss: 0.6930 - val_acc: 0.5117\n",
      "Epoch 23/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6846 - acc: 0.5341 - val_loss: 0.6935 - val_acc: 0.5036\n",
      "Epoch 24/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6806 - acc: 0.5591 - val_loss: 0.6934 - val_acc: 0.5004\n",
      "Epoch 25/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6789 - acc: 0.5508 - val_loss: 0.6937 - val_acc: 0.5044\n",
      "Epoch 26/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6811 - acc: 0.5456 - val_loss: 0.6933 - val_acc: 0.5165\n",
      "Epoch 27/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6766 - acc: 0.5613 - val_loss: 0.6934 - val_acc: 0.5117\n",
      "Epoch 28/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6748 - acc: 0.5621 - val_loss: 0.6934 - val_acc: 0.5052\n",
      "Epoch 29/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6703 - acc: 0.5787 - val_loss: 0.6933 - val_acc: 0.5093\n",
      "Epoch 30/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6692 - acc: 0.5847 - val_loss: 0.6937 - val_acc: 0.5052\n",
      "Epoch 31/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6715 - acc: 0.5714 - val_loss: 0.6935 - val_acc: 0.5165\n",
      "Epoch 32/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6695 - acc: 0.5696 - val_loss: 0.6936 - val_acc: 0.5125\n",
      "Epoch 33/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6688 - acc: 0.5700 - val_loss: 0.6937 - val_acc: 0.5222\n",
      "Epoch 34/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6659 - acc: 0.5706 - val_loss: 0.6940 - val_acc: 0.5109\n",
      "Epoch 35/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6647 - acc: 0.5837 - val_loss: 0.6941 - val_acc: 0.5198\n",
      "Epoch 36/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6625 - acc: 0.5916 - val_loss: 0.6953 - val_acc: 0.5125\n",
      "Epoch 37/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6624 - acc: 0.5978 - val_loss: 0.6947 - val_acc: 0.5206\n",
      "Epoch 38/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6593 - acc: 0.5978 - val_loss: 0.6956 - val_acc: 0.5061\n",
      "Epoch 39/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6568 - acc: 0.6119 - val_loss: 0.6955 - val_acc: 0.5117\n",
      "Epoch 40/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6548 - acc: 0.6051 - val_loss: 0.6964 - val_acc: 0.5052\n",
      "Epoch 41/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6586 - acc: 0.5998 - val_loss: 0.6958 - val_acc: 0.5109\n",
      "Epoch 42/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6512 - acc: 0.6093 - val_loss: 0.6963 - val_acc: 0.5141\n",
      "Epoch 43/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6487 - acc: 0.6101 - val_loss: 0.6969 - val_acc: 0.5101\n",
      "Epoch 44/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6491 - acc: 0.6208 - val_loss: 0.6973 - val_acc: 0.5125\n",
      "Epoch 45/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6453 - acc: 0.6123 - val_loss: 0.6978 - val_acc: 0.5101\n",
      "Epoch 46/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6428 - acc: 0.6289 - val_loss: 0.6986 - val_acc: 0.5044\n",
      "Epoch 47/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6413 - acc: 0.6309 - val_loss: 0.6992 - val_acc: 0.5109\n",
      "Epoch 48/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6400 - acc: 0.6238 - val_loss: 0.6996 - val_acc: 0.5101\n",
      "Epoch 49/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6412 - acc: 0.6329 - val_loss: 0.7017 - val_acc: 0.5020\n",
      "Epoch 50/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6353 - acc: 0.6345 - val_loss: 0.7016 - val_acc: 0.5141\n",
      "Epoch 51/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6319 - acc: 0.6349 - val_loss: 0.7032 - val_acc: 0.5044\n",
      "Epoch 52/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6351 - acc: 0.6325 - val_loss: 0.7027 - val_acc: 0.5125\n",
      "Epoch 53/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6311 - acc: 0.6384 - val_loss: 0.7040 - val_acc: 0.4980\n",
      "Epoch 54/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6286 - acc: 0.6478 - val_loss: 0.7039 - val_acc: 0.5141\n",
      "Epoch 55/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6270 - acc: 0.6547 - val_loss: 0.7065 - val_acc: 0.4964\n",
      "Epoch 56/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6285 - acc: 0.6505 - val_loss: 0.7053 - val_acc: 0.5157\n",
      "Epoch 57/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6247 - acc: 0.6480 - val_loss: 0.7062 - val_acc: 0.5085\n",
      "Epoch 58/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6249 - acc: 0.6547 - val_loss: 0.7070 - val_acc: 0.5085\n",
      "Epoch 59/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6154 - acc: 0.6650 - val_loss: 0.7086 - val_acc: 0.5109\n",
      "Epoch 60/60\n",
      "4958/4958 [==============================] - 0s - loss: 0.6189 - acc: 0.6581 - val_loss: 0.7112 - val_acc: 0.5012\n",
      "1239/1239 [==============================] - 0s\n",
      "0\n",
      "acc:0.5027\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.50      0.45      0.47       595\n",
      "         up       0.54      0.59      0.56       646\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1241\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.44      0.27      0.33       594\n",
      "         up       0.50      0.68      0.58       645\n",
      "\n",
      "avg / total       0.47      0.48      0.46      1239\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.48      0.36      0.41       594\n",
      "         up       0.52      0.65      0.58       645\n",
      "\n",
      "avg / total       0.50      0.51      0.50      1239\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.47      0.34      0.39       594\n",
      "         up       0.52      0.65      0.58       645\n",
      "\n",
      "avg / total       0.50      0.50      0.49      1239\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.48      0.52      0.50       594\n",
      "         up       0.52      0.49      0.50       645\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1239\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_pred = np.zeros(6197) #record pred value\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "acc = []\n",
    "num_epochs = 60 #37\n",
    "history = ''\n",
    "for i in range(1):\n",
    "    for train_idx, test_idx in skf.split(X, Y):\n",
    "        print (\"Running Fold\")\n",
    "        model = create_model()\n",
    "        if with_news:\n",
    "            x = X.toarray()\n",
    "        else:\n",
    "            x = X\n",
    "        y = np_utils.to_categorical(Y, num_classes)\n",
    "        report = model.fit(x[train_idx], y[train_idx], \n",
    "                        epochs=num_epochs,\n",
    "                        validation_data =(x[test_idx], y[test_idx]),\n",
    "                        batch_size=7000,\n",
    "                        verbose = 1)\n",
    "        acc.append(model.evaluate(x[test_idx], y[test_idx], batch_size=2000)[1])\n",
    "        history +=show_result(x[test_idx], Y[test_idx], binary_pred, test_idx)\n",
    "    print(i)\n",
    "print('acc:%.4f'%(np.mean(acc)))\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = create_model()\n",
    "# x = X.toarray()\n",
    "# y = np_utils.to_categorical(Y, num_classes)\n",
    "# model.fit(x, y, \n",
    "#         epochs=num_epochs,\n",
    "#         batch_size=7000,\n",
    "#         verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model.h5')\n",
    "# joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output['gbp_gradient_binary_pred'] = binary_pred\n",
    "# output.to_pickle('output.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
