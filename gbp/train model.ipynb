{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MySQLdb\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, StratifiedKFold, KFold\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Merge, Input, Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.externals import joblib\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary(Y):\n",
    "    Y[np.where(Y > 0)] = 1\n",
    "    Y[np.where(Y <= 0)] = 0\n",
    "    Y = Y.astype('int64')\n",
    "    f = np.bincount(Y)\n",
    "    print(f/np.sum(f))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shift(data, shift_offset=2):\n",
    "    g = data.as_matrix(columns=['gbp_gradient']).reshape(-1)\n",
    "    for i in range(1, shift_offset+1):\n",
    "        data['gbp_gradient_p_'+str(i)] = shift(g, i, cval=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('newstitle_gbp(uk-news,world,business,technology,money_2000-2016).csv')\n",
    "data['gbp_gradient'] = np.gradient(data.as_matrix(columns=['gbp']).reshape(-1))\n",
    "get_shift(data, shift_offset=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>gbp</th>\n",
       "      <th>gbp_gradient</th>\n",
       "      <th>gbp_gradient_p_1</th>\n",
       "      <th>gbp_gradient_p_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>946656000</td>\n",
       "      <td>Big Macs, small horizons Y2K force outstrips s...</td>\n",
       "      <td>1.615700</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>946742400</td>\n",
       "      <td>Cock-fighting? Yes please Samaritans split ove...</td>\n",
       "      <td>1.615000</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>946828800</td>\n",
       "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
       "      <td>1.626724</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>-0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>946915200</td>\n",
       "      <td>Leader: German sleaze inquiry Burundi peace in...</td>\n",
       "      <td>1.637002</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.005512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>947001600</td>\n",
       "      <td>Women in record South Pole walk Deaths no coin...</td>\n",
       "      <td>1.638331</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.011001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                                              title       gbp  \\\n",
       "0  946656000  Big Macs, small horizons Y2K force outstrips s...  1.615700   \n",
       "1  946742400  Cock-fighting? Yes please Samaritans split ove...  1.615000   \n",
       "2  946828800  Hunters threaten Jospin with new battle of the...  1.626724   \n",
       "3  946915200  Leader: German sleaze inquiry Burundi peace in...  1.637002   \n",
       "4  947001600  Women in record South Pole walk Deaths no coin...  1.638331   \n",
       "\n",
       "   gbp_gradient  gbp_gradient_p_1  gbp_gradient_p_2  \n",
       "0     -0.000700          3.000000          3.000000  \n",
       "1      0.005512         -0.000700          3.000000  \n",
       "2      0.011001          0.005512         -0.000700  \n",
       "3      0.005804          0.011001          0.005512  \n",
       "4      0.005379          0.005804          0.011001  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.480071  0.519929]\n"
     ]
    }
   ],
   "source": [
    "Y = data.as_matrix(columns=['gbp_gradient']).reshape(-1)\n",
    "#Y = shift(Y, -3, cval=3)\n",
    "Y = binary(Y)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4795869  0.5204131]\n",
      "[ 0.4795869  0.5204131]\n"
     ]
    }
   ],
   "source": [
    "X_p = data.as_matrix(columns=['gbp_gradient_p_1', 'gbp_gradient_p_2'])\n",
    "X_p[:,0] = binary(X_p[:,0])\n",
    "X_p[:,1] = binary(X_p[:,1])\n",
    "# X_p = data.as_matrix(columns=['gbp_gradient','gbp_gradient_p_1'])\n",
    "# X_p[:,0] = binary(X_p[:,0])\n",
    "# X_p[:,1] = binary(X_p[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6197x2730 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 476823 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_name = 'title'\n",
    "vectorizer = TfidfVectorizer(min_df=50, ngram_range=(1, 4))\n",
    "X = vectorizer.fit_transform(data[feature_name].tolist())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with_news = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if with_news:\n",
    "    X = hstack([X, X.power(2), X.power(3), X.power(4), X_p])\n",
    "else:\n",
    "    X = X_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='tanh', input_shape=(X.shape[1],), bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='tanh', bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='tanh', bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='tanh', bias_initializer='RandomNormal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='RMSprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output = pd.DataFrame()\n",
    "# output['timestamp'] = data['timestamp']\n",
    "# output['gbp'] = data['gbp']\n",
    "# output['gbp_gradient'] = data['gbp_gradient']\n",
    "# output['gbp_gradient_binary'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_result(X_test, y_test, binary_pred, test_idx):\n",
    "    target_names = ['decline','up']\n",
    "    pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    binary_pred[test_idx] = pred\n",
    "    #print(confusion_matrix(y_test, pred, labels=[0,1]))\n",
    "    mat = confusion_matrix(y_test, pred, labels=[0,1])\n",
    "    report = classification_report(y_test, pred, target_names=target_names) + '\\n'\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_216 (Dense)            (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_173 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_174 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_175 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_176 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4957 samples, validate on 1240 samples\n",
      "Epoch 1/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.7073 - acc: 0.5150 - val_loss: 0.6926 - val_acc: 0.5202\n",
      "Epoch 2/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.7015 - acc: 0.5128 - val_loss: 0.6920 - val_acc: 0.5202\n",
      "Epoch 3/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.7012 - acc: 0.5090 - val_loss: 0.6916 - val_acc: 0.5202\n",
      "Epoch 4/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6990 - acc: 0.5142 - val_loss: 0.6913 - val_acc: 0.5202\n",
      "Epoch 5/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6946 - acc: 0.5211 - val_loss: 0.6911 - val_acc: 0.5202\n",
      "Epoch 6/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6960 - acc: 0.5150 - val_loss: 0.6908 - val_acc: 0.5210\n",
      "Epoch 7/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6951 - acc: 0.5211 - val_loss: 0.6905 - val_acc: 0.5210\n",
      "Epoch 8/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6925 - acc: 0.5148 - val_loss: 0.6903 - val_acc: 0.5218\n",
      "Epoch 9/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6910 - acc: 0.5283 - val_loss: 0.6900 - val_acc: 0.5226\n",
      "Epoch 10/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6918 - acc: 0.5245 - val_loss: 0.6896 - val_acc: 0.5250\n",
      "Epoch 11/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6914 - acc: 0.5263 - val_loss: 0.6892 - val_acc: 0.5250\n",
      "Epoch 12/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6894 - acc: 0.5326 - val_loss: 0.6889 - val_acc: 0.5290\n",
      "Epoch 13/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6896 - acc: 0.5330 - val_loss: 0.6885 - val_acc: 0.5306\n",
      "Epoch 14/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6876 - acc: 0.5344 - val_loss: 0.6880 - val_acc: 0.5290\n",
      "Epoch 15/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6857 - acc: 0.5457 - val_loss: 0.6875 - val_acc: 0.5371\n",
      "Epoch 16/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6851 - acc: 0.5429 - val_loss: 0.6870 - val_acc: 0.5355\n",
      "Epoch 17/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6827 - acc: 0.5465 - val_loss: 0.6865 - val_acc: 0.5347\n",
      "Epoch 18/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6800 - acc: 0.5576 - val_loss: 0.6860 - val_acc: 0.5516\n",
      "Epoch 19/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6806 - acc: 0.5606 - val_loss: 0.6855 - val_acc: 0.5702\n",
      "Epoch 20/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6798 - acc: 0.5574 - val_loss: 0.6848 - val_acc: 0.5492\n",
      "Epoch 21/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6794 - acc: 0.5429 - val_loss: 0.6842 - val_acc: 0.5629\n",
      "Epoch 22/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6742 - acc: 0.5709 - val_loss: 0.6836 - val_acc: 0.5927\n",
      "Epoch 23/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6755 - acc: 0.5657 - val_loss: 0.6830 - val_acc: 0.5976\n",
      "Epoch 24/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6734 - acc: 0.5701 - val_loss: 0.6824 - val_acc: 0.5879\n",
      "Epoch 25/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6716 - acc: 0.5772 - val_loss: 0.6818 - val_acc: 0.6040\n",
      "Epoch 26/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6706 - acc: 0.5792 - val_loss: 0.6811 - val_acc: 0.5984\n",
      "Epoch 27/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6676 - acc: 0.5885 - val_loss: 0.6805 - val_acc: 0.6145\n",
      "Epoch 28/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6676 - acc: 0.5868 - val_loss: 0.6798 - val_acc: 0.5952\n",
      "Epoch 29/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6647 - acc: 0.5887 - val_loss: 0.6792 - val_acc: 0.6161\n",
      "Epoch 30/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6628 - acc: 0.5959 - val_loss: 0.6785 - val_acc: 0.6040\n",
      "Epoch 31/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6566 - acc: 0.6088 - val_loss: 0.6781 - val_acc: 0.6242\n",
      "Epoch 32/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6575 - acc: 0.6038 - val_loss: 0.6772 - val_acc: 0.6177\n",
      "Epoch 33/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6553 - acc: 0.6163 - val_loss: 0.6767 - val_acc: 0.6242\n",
      "Epoch 34/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6566 - acc: 0.6109 - val_loss: 0.6763 - val_acc: 0.6234\n",
      "Epoch 35/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6493 - acc: 0.6173 - val_loss: 0.6756 - val_acc: 0.5984\n",
      "Epoch 36/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6520 - acc: 0.6092 - val_loss: 0.6750 - val_acc: 0.6258\n",
      "Epoch 37/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6510 - acc: 0.6173 - val_loss: 0.6743 - val_acc: 0.6218\n",
      "1240/1240 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_221 (Dense)            (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_177 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_222 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_178 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_223 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_179 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_180 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4957 samples, validate on 1240 samples\n",
      "Epoch 1/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.7040 - acc: 0.5017 - val_loss: 0.6923 - val_acc: 0.5218\n",
      "Epoch 2/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.7038 - acc: 0.4866 - val_loss: 0.6924 - val_acc: 0.5355\n",
      "Epoch 3/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6992 - acc: 0.4957 - val_loss: 0.6915 - val_acc: 0.5202\n",
      "Epoch 4/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6948 - acc: 0.5168 - val_loss: 0.6911 - val_acc: 0.5210\n",
      "Epoch 5/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6945 - acc: 0.5168 - val_loss: 0.6908 - val_acc: 0.5266\n",
      "Epoch 6/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6956 - acc: 0.5146 - val_loss: 0.6902 - val_acc: 0.5202\n",
      "Epoch 7/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6917 - acc: 0.5213 - val_loss: 0.6899 - val_acc: 0.5242\n",
      "Epoch 8/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6890 - acc: 0.5289 - val_loss: 0.6893 - val_acc: 0.5242\n",
      "Epoch 9/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6887 - acc: 0.5328 - val_loss: 0.6890 - val_acc: 0.5581\n",
      "Epoch 10/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6883 - acc: 0.5312 - val_loss: 0.6881 - val_acc: 0.5282\n",
      "Epoch 11/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6881 - acc: 0.5419 - val_loss: 0.6876 - val_acc: 0.5226\n",
      "Epoch 12/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6869 - acc: 0.5342 - val_loss: 0.6872 - val_acc: 0.5565\n",
      "Epoch 13/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6843 - acc: 0.5447 - val_loss: 0.6865 - val_acc: 0.5532\n",
      "Epoch 14/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6852 - acc: 0.5334 - val_loss: 0.6858 - val_acc: 0.5669\n",
      "Epoch 15/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6836 - acc: 0.5404 - val_loss: 0.6852 - val_acc: 0.5597\n",
      "Epoch 16/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6775 - acc: 0.5663 - val_loss: 0.6845 - val_acc: 0.5750\n",
      "Epoch 17/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6792 - acc: 0.5606 - val_loss: 0.6839 - val_acc: 0.5863\n",
      "Epoch 18/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6767 - acc: 0.5641 - val_loss: 0.6833 - val_acc: 0.5774\n",
      "Epoch 19/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6787 - acc: 0.5515 - val_loss: 0.6830 - val_acc: 0.6016\n",
      "Epoch 20/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6752 - acc: 0.5737 - val_loss: 0.6821 - val_acc: 0.5685\n",
      "Epoch 21/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6747 - acc: 0.5636 - val_loss: 0.6813 - val_acc: 0.5815\n",
      "Epoch 22/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6722 - acc: 0.5731 - val_loss: 0.6812 - val_acc: 0.6113\n",
      "Epoch 23/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6704 - acc: 0.5838 - val_loss: 0.6797 - val_acc: 0.5968\n",
      "Epoch 24/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6712 - acc: 0.5772 - val_loss: 0.6794 - val_acc: 0.6048\n",
      "Epoch 25/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6676 - acc: 0.5820 - val_loss: 0.6783 - val_acc: 0.6048\n",
      "Epoch 26/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6654 - acc: 0.5824 - val_loss: 0.6777 - val_acc: 0.6137\n",
      "Epoch 27/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6647 - acc: 0.5866 - val_loss: 0.6773 - val_acc: 0.6073\n",
      "Epoch 28/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6641 - acc: 0.6002 - val_loss: 0.6760 - val_acc: 0.6016\n",
      "Epoch 29/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6600 - acc: 0.6076 - val_loss: 0.6756 - val_acc: 0.6089\n",
      "Epoch 30/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6560 - acc: 0.6111 - val_loss: 0.6745 - val_acc: 0.6121\n",
      "Epoch 31/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6601 - acc: 0.5945 - val_loss: 0.6738 - val_acc: 0.6105\n",
      "Epoch 32/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6555 - acc: 0.6062 - val_loss: 0.6739 - val_acc: 0.6000\n",
      "Epoch 33/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6527 - acc: 0.6203 - val_loss: 0.6731 - val_acc: 0.6040\n",
      "Epoch 34/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6535 - acc: 0.6121 - val_loss: 0.6722 - val_acc: 0.6081\n",
      "Epoch 35/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6538 - acc: 0.6024 - val_loss: 0.6717 - val_acc: 0.6065\n",
      "Epoch 36/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6491 - acc: 0.6147 - val_loss: 0.6711 - val_acc: 0.6089\n",
      "Epoch 37/37\n",
      "4957/4957 [==============================] - 0s - loss: 0.6486 - acc: 0.6191 - val_loss: 0.6708 - val_acc: 0.6097\n",
      "1240/1240 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_226 (Dense)            (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_181 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_182 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_183 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_184 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_230 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7071 - acc: 0.4925 - val_loss: 0.6911 - val_acc: 0.5327\n",
      "Epoch 2/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7003 - acc: 0.5054 - val_loss: 0.6908 - val_acc: 0.5351\n",
      "Epoch 3/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6931 - acc: 0.5180 - val_loss: 0.6901 - val_acc: 0.5400\n",
      "Epoch 4/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6933 - acc: 0.5192 - val_loss: 0.6894 - val_acc: 0.5553\n",
      "Epoch 5/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6902 - acc: 0.5288 - val_loss: 0.6892 - val_acc: 0.5666\n",
      "Epoch 6/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6877 - acc: 0.5379 - val_loss: 0.6881 - val_acc: 0.5682\n",
      "Epoch 7/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6859 - acc: 0.5456 - val_loss: 0.6875 - val_acc: 0.5803\n",
      "Epoch 8/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6837 - acc: 0.5462 - val_loss: 0.6867 - val_acc: 0.5755\n",
      "Epoch 9/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6815 - acc: 0.5593 - val_loss: 0.6862 - val_acc: 0.5940\n",
      "Epoch 10/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6813 - acc: 0.5581 - val_loss: 0.6853 - val_acc: 0.5851\n",
      "Epoch 11/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6790 - acc: 0.5534 - val_loss: 0.6848 - val_acc: 0.5997\n",
      "Epoch 12/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6780 - acc: 0.5674 - val_loss: 0.6838 - val_acc: 0.5892\n",
      "Epoch 13/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6770 - acc: 0.5672 - val_loss: 0.6831 - val_acc: 0.5924\n",
      "Epoch 14/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6744 - acc: 0.5712 - val_loss: 0.6826 - val_acc: 0.6045\n",
      "Epoch 15/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6759 - acc: 0.5696 - val_loss: 0.6821 - val_acc: 0.5997\n",
      "Epoch 16/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6699 - acc: 0.5831 - val_loss: 0.6812 - val_acc: 0.5956\n",
      "Epoch 17/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6706 - acc: 0.5877 - val_loss: 0.6802 - val_acc: 0.5948\n",
      "Epoch 18/37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4958/4958 [==============================] - 0s - loss: 0.6645 - acc: 0.5889 - val_loss: 0.6794 - val_acc: 0.5948\n",
      "Epoch 19/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6640 - acc: 0.5954 - val_loss: 0.6789 - val_acc: 0.6029\n",
      "Epoch 20/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6649 - acc: 0.5922 - val_loss: 0.6780 - val_acc: 0.5948\n",
      "Epoch 21/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6590 - acc: 0.6129 - val_loss: 0.6775 - val_acc: 0.6094\n",
      "Epoch 22/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6573 - acc: 0.6158 - val_loss: 0.6769 - val_acc: 0.6118\n",
      "Epoch 23/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6555 - acc: 0.6172 - val_loss: 0.6760 - val_acc: 0.6069\n",
      "Epoch 24/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6503 - acc: 0.6196 - val_loss: 0.6751 - val_acc: 0.6021\n",
      "Epoch 25/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6525 - acc: 0.6216 - val_loss: 0.6751 - val_acc: 0.6077\n",
      "Epoch 26/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6484 - acc: 0.6319 - val_loss: 0.6735 - val_acc: 0.6029\n",
      "Epoch 27/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6469 - acc: 0.6303 - val_loss: 0.6725 - val_acc: 0.6110\n",
      "Epoch 28/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6465 - acc: 0.6370 - val_loss: 0.6721 - val_acc: 0.6102\n",
      "Epoch 29/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6460 - acc: 0.6347 - val_loss: 0.6717 - val_acc: 0.6126\n",
      "Epoch 30/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6420 - acc: 0.6418 - val_loss: 0.6710 - val_acc: 0.6053\n",
      "Epoch 31/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6433 - acc: 0.6295 - val_loss: 0.6706 - val_acc: 0.6150\n",
      "Epoch 32/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6349 - acc: 0.6515 - val_loss: 0.6697 - val_acc: 0.6134\n",
      "Epoch 33/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6357 - acc: 0.6438 - val_loss: 0.6693 - val_acc: 0.6126\n",
      "Epoch 34/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6324 - acc: 0.6571 - val_loss: 0.6692 - val_acc: 0.6158\n",
      "Epoch 35/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6324 - acc: 0.6535 - val_loss: 0.6684 - val_acc: 0.6150\n",
      "Epoch 36/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6311 - acc: 0.6549 - val_loss: 0.6678 - val_acc: 0.6086\n",
      "Epoch 37/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6285 - acc: 0.6595 - val_loss: 0.6674 - val_acc: 0.6110\n",
      "1239/1239 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_231 (Dense)            (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_232 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_233 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_187 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_234 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_188 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_235 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7322 - acc: 0.5137 - val_loss: 0.6908 - val_acc: 0.5198\n",
      "Epoch 2/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7203 - acc: 0.5042 - val_loss: 0.6898 - val_acc: 0.5198\n",
      "Epoch 3/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7082 - acc: 0.5198 - val_loss: 0.6889 - val_acc: 0.5198\n",
      "Epoch 4/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7073 - acc: 0.5157 - val_loss: 0.6880 - val_acc: 0.5230\n",
      "Epoch 5/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7033 - acc: 0.5248 - val_loss: 0.6873 - val_acc: 0.5222\n",
      "Epoch 6/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6940 - acc: 0.5256 - val_loss: 0.6865 - val_acc: 0.5262\n",
      "Epoch 7/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6963 - acc: 0.5331 - val_loss: 0.6861 - val_acc: 0.5230\n",
      "Epoch 8/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6948 - acc: 0.5296 - val_loss: 0.6854 - val_acc: 0.5238\n",
      "Epoch 9/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6933 - acc: 0.5294 - val_loss: 0.6848 - val_acc: 0.5432\n",
      "Epoch 10/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6895 - acc: 0.5305 - val_loss: 0.6843 - val_acc: 0.5851\n",
      "Epoch 11/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6881 - acc: 0.5391 - val_loss: 0.6838 - val_acc: 0.5706\n",
      "Epoch 12/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6878 - acc: 0.5478 - val_loss: 0.6832 - val_acc: 0.5948\n",
      "Epoch 13/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6829 - acc: 0.5496 - val_loss: 0.6825 - val_acc: 0.5916\n",
      "Epoch 14/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6837 - acc: 0.5563 - val_loss: 0.6820 - val_acc: 0.6086\n",
      "Epoch 15/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6821 - acc: 0.5575 - val_loss: 0.6816 - val_acc: 0.6166\n",
      "Epoch 16/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6797 - acc: 0.5581 - val_loss: 0.6808 - val_acc: 0.6037\n",
      "Epoch 17/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6814 - acc: 0.5518 - val_loss: 0.6801 - val_acc: 0.6013\n",
      "Epoch 18/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6801 - acc: 0.5518 - val_loss: 0.6797 - val_acc: 0.6190\n",
      "Epoch 19/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6778 - acc: 0.5706 - val_loss: 0.6791 - val_acc: 0.6303\n",
      "Epoch 20/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6781 - acc: 0.5609 - val_loss: 0.6782 - val_acc: 0.6142\n",
      "Epoch 21/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6737 - acc: 0.5710 - val_loss: 0.6778 - val_acc: 0.6400\n",
      "Epoch 22/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6789 - acc: 0.5555 - val_loss: 0.6768 - val_acc: 0.6182\n",
      "Epoch 23/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6713 - acc: 0.5696 - val_loss: 0.6760 - val_acc: 0.6279\n",
      "Epoch 24/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6708 - acc: 0.5756 - val_loss: 0.6755 - val_acc: 0.6295\n",
      "Epoch 25/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6664 - acc: 0.5809 - val_loss: 0.6749 - val_acc: 0.6312\n",
      "Epoch 26/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6681 - acc: 0.5879 - val_loss: 0.6743 - val_acc: 0.6344\n",
      "Epoch 27/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6648 - acc: 0.5819 - val_loss: 0.6735 - val_acc: 0.6328\n",
      "Epoch 28/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6580 - acc: 0.5976 - val_loss: 0.6724 - val_acc: 0.6320\n",
      "Epoch 29/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6602 - acc: 0.5996 - val_loss: 0.6717 - val_acc: 0.6376\n",
      "Epoch 30/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6594 - acc: 0.5912 - val_loss: 0.6711 - val_acc: 0.6312\n",
      "Epoch 31/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6581 - acc: 0.5956 - val_loss: 0.6706 - val_acc: 0.6336\n",
      "Epoch 32/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6533 - acc: 0.6010 - val_loss: 0.6697 - val_acc: 0.6303\n",
      "Epoch 33/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6534 - acc: 0.6119 - val_loss: 0.6693 - val_acc: 0.6336\n",
      "Epoch 34/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6495 - acc: 0.6134 - val_loss: 0.6687 - val_acc: 0.6360\n",
      "Epoch 35/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6480 - acc: 0.6158 - val_loss: 0.6681 - val_acc: 0.6328\n",
      "Epoch 36/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6485 - acc: 0.6081 - val_loss: 0.6676 - val_acc: 0.6207\n",
      "Epoch 37/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6511 - acc: 0.6039 - val_loss: 0.6674 - val_acc: 0.6360\n",
      "1239/1239 [==============================] - 0s\n",
      "Running Fold\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_236 (Dense)            (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_237 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_191 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_192 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4958 samples, validate on 1239 samples\n",
      "Epoch 1/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7643 - acc: 0.5143 - val_loss: 0.6938 - val_acc: 0.5198\n",
      "Epoch 2/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7322 - acc: 0.5157 - val_loss: 0.6923 - val_acc: 0.5198\n",
      "Epoch 3/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7171 - acc: 0.5204 - val_loss: 0.6898 - val_acc: 0.5198\n",
      "Epoch 4/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7126 - acc: 0.5250 - val_loss: 0.6872 - val_acc: 0.5214\n",
      "Epoch 5/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7035 - acc: 0.5266 - val_loss: 0.6857 - val_acc: 0.5206\n",
      "Epoch 6/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.7086 - acc: 0.5141 - val_loss: 0.6850 - val_acc: 0.5206\n",
      "Epoch 7/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6988 - acc: 0.5339 - val_loss: 0.6835 - val_acc: 0.5287\n",
      "Epoch 8/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6952 - acc: 0.5385 - val_loss: 0.6828 - val_acc: 0.5254\n",
      "Epoch 9/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6943 - acc: 0.5379 - val_loss: 0.6815 - val_acc: 0.5593\n",
      "Epoch 10/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6894 - acc: 0.5454 - val_loss: 0.6805 - val_acc: 0.5617\n",
      "Epoch 11/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6867 - acc: 0.5462 - val_loss: 0.6795 - val_acc: 0.5601\n",
      "Epoch 12/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6842 - acc: 0.5494 - val_loss: 0.6784 - val_acc: 0.6029\n",
      "Epoch 13/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6843 - acc: 0.5482 - val_loss: 0.6772 - val_acc: 0.6102\n",
      "Epoch 14/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6813 - acc: 0.5551 - val_loss: 0.6761 - val_acc: 0.6037\n",
      "Epoch 15/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6803 - acc: 0.5520 - val_loss: 0.6749 - val_acc: 0.6142\n",
      "Epoch 16/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6818 - acc: 0.5490 - val_loss: 0.6742 - val_acc: 0.6215\n",
      "Epoch 17/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6829 - acc: 0.5522 - val_loss: 0.6734 - val_acc: 0.6279\n",
      "Epoch 18/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6760 - acc: 0.5635 - val_loss: 0.6723 - val_acc: 0.6295\n",
      "Epoch 19/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6714 - acc: 0.5748 - val_loss: 0.6713 - val_acc: 0.6287\n",
      "Epoch 20/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6698 - acc: 0.5815 - val_loss: 0.6711 - val_acc: 0.6416\n",
      "Epoch 21/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6726 - acc: 0.5823 - val_loss: 0.6697 - val_acc: 0.6295\n",
      "Epoch 22/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6708 - acc: 0.5827 - val_loss: 0.6693 - val_acc: 0.6400\n",
      "Epoch 23/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6678 - acc: 0.5770 - val_loss: 0.6680 - val_acc: 0.6328\n",
      "Epoch 24/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6640 - acc: 0.5920 - val_loss: 0.6673 - val_acc: 0.6433\n",
      "Epoch 25/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6625 - acc: 0.5869 - val_loss: 0.6661 - val_acc: 0.6360\n",
      "Epoch 26/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6618 - acc: 0.5843 - val_loss: 0.6651 - val_acc: 0.6368\n",
      "Epoch 27/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6571 - acc: 0.5990 - val_loss: 0.6640 - val_acc: 0.6368\n",
      "Epoch 28/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6483 - acc: 0.6184 - val_loss: 0.6634 - val_acc: 0.6352\n",
      "Epoch 29/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6529 - acc: 0.6025 - val_loss: 0.6626 - val_acc: 0.6465\n",
      "Epoch 30/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6560 - acc: 0.6077 - val_loss: 0.6617 - val_acc: 0.6320\n",
      "Epoch 31/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6509 - acc: 0.6148 - val_loss: 0.6610 - val_acc: 0.6465\n",
      "Epoch 32/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6442 - acc: 0.6200 - val_loss: 0.6599 - val_acc: 0.6360\n",
      "Epoch 33/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6486 - acc: 0.6041 - val_loss: 0.6596 - val_acc: 0.6392\n",
      "Epoch 34/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6436 - acc: 0.6170 - val_loss: 0.6586 - val_acc: 0.6360\n",
      "Epoch 35/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6414 - acc: 0.6232 - val_loss: 0.6587 - val_acc: 0.6336\n",
      "Epoch 36/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6373 - acc: 0.6232 - val_loss: 0.6574 - val_acc: 0.6336\n",
      "Epoch 37/37\n",
      "4958/4958 [==============================] - 0s - loss: 0.6365 - acc: 0.6400 - val_loss: 0.6575 - val_acc: 0.6344\n",
      "1239/1239 [==============================] - 0s\n",
      "0\n",
      "acc:0.6226\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.64      0.48      0.55       595\n",
      "         up       0.61      0.75      0.67       645\n",
      "\n",
      "avg / total       0.63      0.62      0.61      1240\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.61      0.53      0.57       595\n",
      "         up       0.61      0.68      0.64       645\n",
      "\n",
      "avg / total       0.61      0.61      0.61      1240\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.61      0.51      0.56       595\n",
      "         up       0.61      0.70      0.65       644\n",
      "\n",
      "avg / total       0.61      0.61      0.61      1239\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.63      0.57      0.60       595\n",
      "         up       0.64      0.69      0.66       644\n",
      "\n",
      "avg / total       0.64      0.64      0.63      1239\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    decline       0.62      0.62      0.62       595\n",
      "         up       0.65      0.65      0.65       644\n",
      "\n",
      "avg / total       0.63      0.63      0.63      1239\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_pred = np.zeros(6197) #record pred value\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "acc = []\n",
    "num_epochs = 37 \n",
    "history = ''\n",
    "for i in range(1):\n",
    "    for train_idx, test_idx in skf.split(X, Y):\n",
    "        print (\"Running Fold\")\n",
    "        model = create_model()\n",
    "        if with_news:\n",
    "            x = X.toarray()\n",
    "        else:\n",
    "            x = X\n",
    "        y = np_utils.to_categorical(Y, num_classes)\n",
    "        report = model.fit(x[train_idx], y[train_idx], \n",
    "                        epochs=num_epochs,\n",
    "                        validation_data =(x[test_idx], y[test_idx]),\n",
    "                        batch_size=7000,\n",
    "                        verbose = 1)\n",
    "        acc.append(model.evaluate(x[test_idx], y[test_idx], batch_size=2000)[1])\n",
    "        history +=show_result(x[test_idx], Y[test_idx], binary_pred, test_idx)\n",
    "    print(i)\n",
    "print('acc:%.4f'%(np.mean(acc)))\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 10)                109230    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 109,582\n",
      "Trainable params: 109,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7303 - acc: 0.4946\n",
      "Epoch 2/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7142 - acc: 0.4994\n",
      "Epoch 3/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7054 - acc: 0.5123\n",
      "Epoch 4/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.7012 - acc: 0.5127\n",
      "Epoch 5/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6990 - acc: 0.5069\n",
      "Epoch 6/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6996 - acc: 0.5125\n",
      "Epoch 7/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6951 - acc: 0.5251\n",
      "Epoch 8/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6963 - acc: 0.5159\n",
      "Epoch 9/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6926 - acc: 0.5270\n",
      "Epoch 10/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6911 - acc: 0.5353\n",
      "Epoch 11/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6922 - acc: 0.5215\n",
      "Epoch 12/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6894 - acc: 0.5351\n",
      "Epoch 13/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6899 - acc: 0.5343\n",
      "Epoch 14/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6879 - acc: 0.5372\n",
      "Epoch 15/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6901 - acc: 0.5317\n",
      "Epoch 16/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6858 - acc: 0.5430\n",
      "Epoch 17/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6845 - acc: 0.5546\n",
      "Epoch 18/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6828 - acc: 0.5491\n",
      "Epoch 19/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6800 - acc: 0.5495\n",
      "Epoch 20/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6800 - acc: 0.5609\n",
      "Epoch 21/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6812 - acc: 0.5506\n",
      "Epoch 22/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6767 - acc: 0.5646\n",
      "Epoch 23/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6781 - acc: 0.5624\n",
      "Epoch 24/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6725 - acc: 0.5800\n",
      "Epoch 25/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6717 - acc: 0.5814\n",
      "Epoch 26/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6708 - acc: 0.5838\n",
      "Epoch 27/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6697 - acc: 0.5761\n",
      "Epoch 28/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6677 - acc: 0.5819\n",
      "Epoch 29/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6672 - acc: 0.5888\n",
      "Epoch 30/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6644 - acc: 0.5788\n",
      "Epoch 31/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6676 - acc: 0.5690\n",
      "Epoch 32/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6655 - acc: 0.5882\n",
      "Epoch 33/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6630 - acc: 0.5909\n",
      "Epoch 34/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6555 - acc: 0.6134\n",
      "Epoch 35/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6604 - acc: 0.5984\n",
      "Epoch 36/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6562 - acc: 0.5958\n",
      "Epoch 37/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6549 - acc: 0.6067\n",
      "Epoch 38/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6525 - acc: 0.6101\n",
      "Epoch 39/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6492 - acc: 0.6161\n",
      "Epoch 40/40\n",
      "6197/6197 [==============================] - 0s - loss: 0.6456 - acc: 0.6298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f35450fe278>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "x = X.toarray()\n",
    "y = np_utils.to_categorical(Y, num_classes)\n",
    "model.fit(x, y, \n",
    "        epochs=num_epochs,\n",
    "        batch_size=7000,\n",
    "        verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save('model.h5')\n",
    "# joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#output['gbp_gradient_binary_pred'] = binary_pred\n",
    "#output.to_pickle('output.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
